# OpenShift (OCP) Usage Data Processing Deep Dive

## Overview

OpenShift Cost Management (OCP) data processing is **fundamentally different** from AWS, Azure, and GCP because it uses a **push model** rather than a pull model. Data is generated by the **cost-mgmt-metrics-operator** running on customer OpenShift clusters and pushed to Red Hat's ingress service via Kafka, rather than being downloaded from cloud storage.

This architecture document provides a comprehensive deep-dive into how Koku processes OpenShift usage data, including:

- How the operator collects metrics from clusters
- Kafka-based data ingestion
- Multiple report type handling (pods, storage, node labels, namespace labels, VMs)
- Cost model application and tag-based cost allocation
- Infrastructure matching (OCP on AWS/Azure/GCP)
- Data summarization and reporting

---

## Key Architectural Differences

### **OCP vs Cloud Provider Data Sources**

| Aspect                   | AWS/Azure/GCP          | OpenShift (OCP)                                       |
| ------------------------ | ---------------------- | ----------------------------------------------------- |
| **Data Source**          | Cloud provider storage | **Customer cluster (Prometheus metrics)**             |
| **Collection Method**    | Billing export         | **cost-mgmt-metrics-operator**                        |
| **Data Transmission**    | Download (pull)        | **Upload via HTTPS POST to Ingress (push)**           |
| **File Format**          | CSV in cloud storage   | **tar.gz with CSV files uploaded to ingress**         |
| **Manifest**             | JSON in cloud storage  | **JSON embedded in tar.gz**                           |
| **Update Frequency**     | Daily/hourly export    | **Every 6 hours (configurable)**                      |
| **Report Types**         | Single billing file    | **5 types: pod, storage, node labels, namespace, VM** |
| **Cost Information**     | Included in data       | **Applied via cost models**                           |
| **Infrastructure Costs** | Direct billing         | **Matched to cloud provider via resource ID**         |
| **Authentication**       | Cloud credentials      | **Sources integration or basic auth**                 |
| **Processing Trigger**   | Scheduled download     | **Kafka message consumption**                         |

**Key Insight:** OCP data is **usage-based** (CPU seconds, memory bytes) not cost-based. Costs are calculated using **cost models** and **infrastructure matching**.

---

## OpenShift Operator Architecture

```
┌──────────────────────────────────────────────────────────────────┐
│  Customer OpenShift Cluster                                       │
│  ┌─────────────────────────────────────────────────────────────┐ │
│  │  Prometheus                                                  │ │
│  │  - Collects pod CPU/memory usage                            │ │
│  │  - Collects storage (PVC) usage                             │ │
│  │  - Collects node capacity metrics                           │ │
│  │  - Stores metrics with labels                               │ │
│  └────────────────┬────────────────────────────────────────────┘ │
│                   │ PromQL queries                                │
│                   ▼                                                │
│  ┌─────────────────────────────────────────────────────────────┐ │
│  │  cost-mgmt-metrics-operator                                 │ │
│  │  - Queries Prometheus for usage metrics                     │ │
│  │  - Generates CSV reports (every 6 hours by default)         │ │
│  │  - Creates manifest.json with cluster info                  │ │
│  │  - Packages as tar.gz                                       │ │
│  │  - Uploads to Red Hat Ingress Service                       │ │
│  └────────────────┬────────────────────────────────────────────┘ │
└────────────────────┼────────────────────────────────────────────┘
                     │ HTTPS POST (tar.gz upload)
                     ▼
┌──────────────────────────────────────────────────────────────────┐
│  Red Hat Ingress Service (platform.redhat.com)                   │
│  - Receives uploaded tar.gz                                      │
│  - Stores in quarantine bucket                                   │
│  - Publishes message to Kafka (platform.upload.announce topic)   │
└────────────────┬─────────────────────────────────────────────────┘
                 │ Kafka message with download URL
                 ▼
┌──────────────────────────────────────────────────────────────────┐
│  Koku/MASU (Consumer)                                            │
│  kafka_msg_handler.py                                            │
│  - Consumes messages from platform.upload.announce               │
│  - Downloads tar.gz from quarantine bucket                       │
│  - Extracts and processes CSV files                              │
│  - Sends validation confirmation back to Kafka                   │
└──────────────────────────────────────────────────────────────────┘
```

---

## Report Types and Structure

OpenShift generates **5 distinct report types** (not a single unified billing file like cloud providers):

### **1. Pod Usage Report** (`pod_usage`)

**Purpose:** CPU and memory usage for pods

**Columns:**
- `report_period_start`, `report_period_end`
- `interval_start`, `interval_end` (hourly interval)
- `namespace`, `pod`, `node`, `resource_id` (EC2/Azure/GCP instance ID)
- `pod_usage_cpu_core_seconds`, `pod_request_cpu_core_seconds`, `pod_limit_cpu_core_seconds`
- `pod_usage_memory_byte_seconds`, `pod_request_memory_byte_seconds`, `pod_limit_memory_byte_seconds`
- `node_capacity_cpu_cores`, `node_capacity_cpu_core_seconds`
- `node_capacity_memory_bytes`, `node_capacity_memory_byte_seconds`
- `pod_labels` (JSON)
- `node_role` (new in operator 4.x)

**Example Row:**
```csv
report_period_start,report_period_end,interval_start,interval_end,namespace,pod,node,resource_id,pod_usage_cpu_core_seconds,pod_request_cpu_core_seconds,pod_limit_cpu_core_seconds,pod_usage_memory_byte_seconds,pod_request_memory_byte_seconds,pod_limit_memory_byte_seconds,node_capacity_cpu_cores,node_capacity_cpu_core_seconds,node_capacity_memory_bytes,node_capacity_memory_byte_seconds,pod_labels,node_role
2025-01-01,2025-02-01,2025-01-15T00:00:00Z,2025-01-15T01:00:00Z,koku-app,koku-worker-7d5f8b9c6d-abc123,ip-10-0-1-100.ec2.internal,i-0abcd1234efgh5678,1234.56,3600.00,7200.00,12345678900,26843545600,53687091200,8,28800,32212254720,115964116992000,"{""app"":""koku"",""env"":""prod""}",worker
```

### **2. Storage Usage Report** (`storage_usage`)

**Purpose:** Persistent volume claim (PVC) usage

**Columns:**
- `report_period_start`, `report_period_end`
- `interval_start`, `interval_end`
- `namespace`, `pod`
- `persistentvolumeclaim`, `persistentvolume`, `storageclass`
- `persistentvolumeclaim_capacity_bytes`, `persistentvolumeclaim_capacity_byte_seconds`
- `volume_request_storage_byte_seconds`, `persistentvolumeclaim_usage_byte_seconds`
- `persistentvolume_labels` (JSON), `persistentvolumeclaim_labels` (JSON)
- `csi_driver`, `csi_volume_handle` (new in operator 4.x)
- `node` (new in operator 4.x)

**Example Row:**
```csv
report_period_start,report_period_end,interval_start,interval_end,namespace,pod,persistentvolumeclaim,persistentvolume,storageclass,persistentvolumeclaim_capacity_bytes,persistentvolumeclaim_capacity_byte_seconds,volume_request_storage_byte_seconds,persistentvolumeclaim_usage_byte_seconds,persistentvolume_labels,persistentvolumeclaim_labels,node,csi_driver,csi_volume_handle
2025-01-01,2025-02-01,2025-01-15T00:00:00Z,2025-01-15T01:00:00Z,koku-app,koku-db-0,postgres-data,pvc-abc123,gp3,107374182400,386547056640000,386547056640000,53687091200000,"{""storage.k8s.io/name"":""gp3""}","{""app"":""postgres""}",ip-10-0-1-100.ec2.internal,ebs.csi.aws.com,vol-0123456789abcdef
```

### **3. Node Labels Report** (`node_labels`)

**Purpose:** Labels attached to nodes

**Columns:**
- `report_period_start`, `report_period_end`
- `interval_start`, `interval_end`
- `node`
- `node_labels` (JSON)

### **4. Namespace Labels Report** (`namespace_labels`)

**Purpose:** Labels attached to namespaces

**Columns:**
- `report_period_start`, `report_period_end`
- `interval_start`, `interval_end`
- `namespace`
- `namespace_labels` (JSON)

### **5. VM Usage Report** (`vm_usage`) [OpenShift Virtualization]

**Purpose:** Virtual machine CPU, memory, disk usage (for kubevirt VMs)

**Columns:**
- `report_period_start`, `report_period_end`
- `interval_start`, `interval_end`
- `namespace`, `node`, `resource_id`
- `vm_uptime_total_seconds`
- `vm_cpu_limit_cores`, `vm_cpu_limit_core_seconds`
- `vm_cpu_request_cores`, `vm_cpu_request_core_seconds`
- `vm_cpu_usage_total_seconds`
- `vm_memory_limit_bytes`, `vm_memory_limit_byte_seconds`
- `vm_memory_request_bytes`, `vm_memory_request_byte_seconds`
- `vm_memory_usage_byte_seconds`
- `vm_disk_allocated_size_byte_seconds`

---

## Data Flow Pipeline

### **Phase 1: Kafka Message Reception and Download**

**File:** `koku/masu/external/kafka_msg_handler.py`

#### **Step 1: Consume Kafka Message**

**Implementation:** See [`listen_for_messages_loop()`](../../koku/masu/external/kafka_msg_handler.py) in `kafka_msg_handler.py`

**Kafka Configuration:**
- Consumer group: `hccm-group`
- Topic: `platform.upload.announce` (`UPLOAD_TOPIC`)
- Max poll interval: 18 minutes
- Auto-commit: Disabled (manual commit after processing)

**Kafka Message Format:**

JSON message containing:
- `request_id` - Unique request identifier
- `account`, `org_id` - Tenant identifiers
- `category` - Must be "hccm" for cost management
- `url` - S3 URL to download tar.gz from quarantine bucket
- `b64_identity` - Base64-encoded identity header
- `timestamp` - Upload timestamp
- `size` - File size in bytes

#### **Step 2: Download Tar.gz from Quarantine Bucket**

**Implementation:** See [`download_payload()`](../../koku/masu/external/kafka_msg_handler.py) in `kafka_msg_handler.py`

**Process:**
1. Create temporary directory in `Config.DATA_DIR`
2. Download file from quarantine S3 URL via HTTP GET
3. Sanitize request_id (alphanumeric only)
4. Write tar.gz to temporary file
5. Return file path for extraction

#### **Step 3: Extract Tar.gz and Parse Manifest**

**Implementation:** See [`extract_payload_contents()`](../../koku/masu/external/kafka_msg_handler.py) in `kafka_msg_handler.py`

**Process:**
1. Open tar.gz file in read mode
2. Extract all contents to temporary directory
3. Search for `manifest.json` in extracted files
4. Raise error if no manifest found
5. Return manifest path

**Manifest Structure:**

JSON file containing:
- `uuid` - Manifest unique identifier
- `cluster_id` - OpenShift cluster UUID
- `date`, `start` - Report date and start time
- `files` - Array of CSV filenames (pod_usage, storage_usage, node_labels, namespace_labels, vm_usage)
- `certified` - Whether cluster is certified OpenShift
- `operator_version` - cost-mgmt-metrics-operator version (e.g., "4.1.0")
- `daily_reports` - Boolean indicating if operator sends daily or cumulative reports
- `cr_status` - Cluster metadata (version, upload config, authentication type)

---

### **Phase 2: CSV Splitting and Daily Archives**

**File:** `koku/masu/external/kafka_msg_handler.py`

OpenShift reports can contain **multiple days** of data. Koku splits them into **daily CSV files** for consistency with cloud provider processing.

#### **Step 1: Detect Report Type**

**Implementation:** See [`detect_type()`](../../koku/masu/util/ocp/common.py) in `masu/util/ocp/common.py`

**Process:**
1. Read first row of CSV to get column names
2. Match column sets against known report types:
   - `CPU_MEM_USAGE_COLUMNS` → `pod_usage`
   - `STORAGE_COLUMNS` → `storage_usage`
   - `node_labels` column → `node_labels`
   - `namespace_labels` column → `namespace_labels`
   - `vm_uptime_total_seconds` column → `vm_usage`
3. Return report type enum and string name

#### **Step 2: Split by Daily Interval**

**Implementation:** See [`divide_csv_daily()`](../../koku/masu/external/kafka_msg_handler.py) in `kafka_msg_handler.py`

**Process:**
1. Read CSV with pandas (pyarrow backend for performance)
2. Detect report type from columns
3. Extract unique days from `interval_start` column (YYYY-MM-DD)
4. Group DataFrame by day
5. For each day:
   - Create filename: `{report_type}.{day}.{manifest_id}.{counter}.csv`
   - Use database transaction to track file counter (prevents collisions)
   - Write daily DataFrame to CSV
   - Track filepath, date, and number of hours
6. Return list of daily file metadata

#### **Step 3: Upload Daily CSVs to S3/MinIO**

**Implementation:** See [`create_daily_archives()`](../../koku/masu/external/kafka_msg_handler.py) in `kafka_msg_handler.py`

**Process:**
1. Check operator version and daily_reports flag
2. **Old operators (cumulative reports):** Don't split, use file as-is
3. **New operators (daily reports):** Split CSV by day using `divide_csv_daily()`
4. For each daily file:
   - Generate S3 path: `{schema}/openshift/csv/{date}/`
   - Upload to S3/MinIO bucket
   - Track metadata (report date, number of hours)
5. Return dictionary mapping filepaths to metadata

**S3/MinIO Path Structure:**
```
org1234567/openshift/csv/
├── pod_usage.2025-01-15.12345.0.csv
├── pod_usage.2025-01-15.12345.1.csv
├── storage_usage.2025-01-15.12345.0.csv
├── node_labels.2025-01-15.12345.0.csv
├── namespace_labels.2025-01-15.12345.0.csv
└── vm_usage.2025-01-15.12345.0.csv
```

---

### **Phase 3: Parquet Conversion**

**File:** `koku/masu/processor/ocp/ocp_report_parquet_processor.py`

#### **Data Type Definitions**

**Implementation:** See [`OCPReportParquetProcessor`](../../koku/masu/processor/ocp/ocp_report_parquet_processor.py) in `ocp_report_parquet_processor.py`

**OCP-Specific Type Handling:**
- **Numeric columns:** CPU/memory usage, requests, limits, effective usage, node capacity, storage metrics, VM metrics
- **Date columns:** `report_period_start`, `report_period_end`, `interval_start`, `interval_end`
- **Table selection:** Based on report type and path (daily vs non-daily)
  - Pod usage → `openshift_pod_usage_line_items_daily`
  - Storage → `openshift_storage_usage_line_items_daily`
  - Node labels → `openshift_node_labels_line_items_daily`
  - Namespace labels → `openshift_namespace_labels_line_items_daily`
  - VM usage → `openshift_vm_usage_line_items_daily`

#### **Calculate Effective Usage**

Unlike cloud providers, OCP must calculate **effective usage** (min of usage and request).

**Implementation:** See [`calculate_effective_usage()`](../../koku/masu/util/ocp/common.py) in `masu/util/ocp/common.py`

**Logic:**
- **CPU effective usage** = `min(pod_usage_cpu_core_seconds, pod_request_cpu_core_seconds)`
- **Memory effective usage** = `min(pod_usage_memory_byte_seconds, pod_request_memory_byte_seconds)`
- Prevents charging for unused requested resources
- Calculated during CSV preprocessing before Parquet conversion

#### **Create Bill Record**

**Implementation:** See [`create_bill()`](../../koku/masu/processor/ocp/ocp_report_parquet_processor.py) in `ocp_report_parquet_processor.py`

**Process:**
1. Parse bill_date to datetime if string
2. Calculate month date range (first to last day of month)
3. Set report_period_end to first of next month
4. Get cluster_id and cluster_alias from provider
5. Create or update `OCPUsageReportPeriod` record with:
   - `cluster_id`
   - `report_period_start`, `report_period_end`
   - `provider_id`
   - `cluster_alias`
6. Return bill object

**Parquet Output Structure:**
```
org1234567/openshift/parquet/
├── openshift_pod_usage_line_items_daily/
│   ├── source=ocp-provider-uuid/
│   │   ├── year=2025/
│   │   │   ├── month=01/
│   │   │   │   ├── day=15/
│   │   │   │   │   ├── part-00000.parquet
│   │   │   │   │   └── part-00001.parquet
├── openshift_storage_usage_line_items_daily/
│   └── source=ocp-provider-uuid/...
├── openshift_node_labels_line_items_daily/...
├── openshift_namespace_labels_line_items_daily/...
└── openshift_vm_usage_line_items_daily/...
```

---

### **Phase 4: Trino Summarization**

**File:** `koku/masu/processor/ocp/ocp_report_parquet_summary_updater.py`
**SQL:** `koku/masu/database/trino_sql/openshift/reporting_ocpusagelineitem_daily_summary.sql`

#### **Orchestration**

**Implementation:** See [`update_summary_tables()`](../../koku/masu/processor/ocp/ocp_report_parquet_summary_updater.py) in `ocp_report_parquet_summary_updater.py`

**Process Flow:**
1. **Normalize dates** and check Parquet date range
2. **Handle table partitions** for UI summary tables
3. **Get report period** for the provider and date
4. **Populate cluster information** (cluster-level metadata)
5. **Process in date chunks** (`TRINO_DATE_STEP`):
   - **DELETE** existing summary data (preserves infrastructure costs)
   - **INSERT** new data via Trino SQL (joins all 5 report types)
   - **Populate UI summary tables** (cost, pod, volume summaries)
6. **Populate label summary tables** (pod and volume labels)
7. **Apply tag mappings** to daily summary
8. **Update report period timestamps**
9. **Check for cloud infrastructure** (triggers OCP-on-Cloud matching)

#### **Trino SQL: Multi-Report Aggregation**

The Trino SQL joins **all 5 report types** to create a unified daily summary.

**Implementation:** See [`reporting_ocpusagelineitem_daily_summary.sql`](../../koku/masu/database/trino_sql/openshift/reporting_ocpusagelineitem_daily_summary.sql) in `trino_sql/openshift/`

**Query Structure:**

**CTEs (Common Table Expressions):**
1. **`cte_pg_enabled_keys`** - Get enabled tag keys from PostgreSQL
2. **`cte_ocp_node_label_line_item_daily`** - Get node labels, filtered by enabled keys using `map_filter`
3. **`cte_ocp_namespace_label_line_item_daily`** - Get namespace labels, filtered by enabled keys
4. **`cte_ocp_node_capacity`** - Daily sum of node CPU and memory capacity from pod usage reports
5. **`cte_ocp_cluster_capacity`** - Daily sum of cluster-wide capacity
6. **`cte_ocp_pod_usage`** - Aggregate pod CPU and memory usage by day, convert seconds → hours, bytes → GB
7. **`cte_ocp_storage_usage`** - Aggregate storage usage by day, convert to gigabyte-months

**Main INSERT Query:**
- **FULL OUTER JOIN** pod usage with storage usage (on date, namespace, node)
- **LEFT JOIN** node labels and namespace labels
- **LEFT JOIN** node capacity and cluster capacity
- **Merge labels** using `map_concat` (pod + node + namespace labels)
- **Determine data_source:** 'Pod' if pod data exists, 'Storage' if PVC exists
- **Convert units:** seconds → hours, bytes → gigabytes
- **Partitioning:** By source, year, month, day

**Key Features:**
1. **Multi-source Join:** Combines pod, storage, node labels, namespace labels
2. **Label Merging:** Merges pod_labels + node_labels + namespace_labels using `map_concat`
3. **Unit Conversion:** Seconds → hours, bytes → gigabytes
4. **Effective Usage:** min(usage, request)
5. **Capacity Aggregation:** Node-level and cluster-level
6. **Data Source Detection:** 'Pod' or 'Storage' based on which data is present

---

### **Phase 5: Cost Model Application**

**File:** `koku/masu/processor/ocp/ocp_cost_model_cost_updater.py`

Unlike cloud providers, **OCP has no cost data in the raw reports**. Costs are calculated using **cost models** defined by users.

#### **Cost Model Structure**

Cost models consist of **infrastructure rates** and **supplementary rates**:

**Infrastructure Rates** (mapped to cloud provider costs):
```json
{
  "cpu_core_usage_per_hour": 0.05,
  "memory_gb_usage_per_hour": 0.01,
  "storage_gb_usage_per_month": 0.10
}
```

**Supplementary Rates** (additional cluster costs):
```json
{
  "node_cost_per_month": 100.0,
  "cluster_cost_per_month": 10000.0,
  "pvc_cost_per_month": 50.0
}
```

**Tag-Based Rates** (assign costs based on labels):
```json
{
  "env": {
    "prod": {"rate": 0.10},
    "dev": {"rate": 0.05},
    "default_value": 0.03
  },
  "team": {
    "platform": {"rate": 0.08},
    "app": {"rate": 0.06}
  }
}
```

#### **Cost Calculation SQL Generation**

The cost updater generates **dynamic SQL CASE statements** for tag-based pricing.

**Implementation:** See [`_build_node_tag_cost_case_statements()`](../../koku/masu/processor/ocp/ocp_cost_model_cost_updater.py) in `ocp_cost_model_cost_updater.py`

**Process:**
1. Define distribution terms (CPU usage ratio, memory usage ratio)
2. For each tag key and its value rates:
   - Build CASE statement for CPU cost allocation
   - Build CASE statement for memory cost allocation
   - Match `pod_labels->>'tag_key'` to rate values
   - Apply default rate for unmatched values (if configured)
3. Return dictionary of SQL CASE statements per tag key
4. Statements calculate: `usage_ratio * amortized_monthly_rate`

**Generated SQL Example:**

The generated SQL updates the daily summary table with dynamically calculated costs based on label values:
- Sets `monthly_cost_type = 'Tag'`
- Calculates `cost_model_cpu_cost` using CASE statement matching label values to rates
- Calculates `cost_model_memory_cost` similarly
- Formula: `(effective_usage / node_capacity) * monthly_rate`
- Filters by date range, report period, and presence of the label key

#### **Cost Distribution**

Cost models support **3 distribution types**:

1. **CPU Distribution** (default): Allocate costs based on CPU usage ratio
2. **Memory Distribution**: Allocate costs based on memory usage ratio
3. **PVC Distribution**: Allocate storage costs

**SQL for Cost Distribution:**

Cost distribution allocates unallocated costs (platform overhead) to namespaces based on their usage ratios. The SQL:
1. Calculates usage ratio: `(pod_effective_usage / node_capacity)`
2. Multiplies by total unallocated infrastructure cost
3. Updates namespace-level infrastructure costs
4. Applies to records with `monthly_cost_type = 'Node'`

---

### **Phase 6: Infrastructure Matching (OCP on AWS/Azure/GCP)**

**File:** `koku/masu/processor/ocp/ocp_cloud_parquet_summary_updater.py`

When an OpenShift cluster runs on cloud infrastructure, Koku **matches OCP nodes to cloud instances** to allocate infrastructure costs.

#### **Infrastructure Mapping Discovery**

**Implementation:** See [`check_cluster_infrastructure()`](../../koku/masu/processor/ocp/ocp_report_parquet_summary_updater.py) in `ocp_report_parquet_summary_updater.py`

**Process:**
1. **Try explicit relationships first:** Check if provider relationships define OCP-on-Cloud connection
2. **Fallback to resource_id matching:** Query Trino to match `resource_id` between OCP and cloud providers
3. **Trigger OCP-Cloud processing:** Call summary updater for AWS/Azure/GCP with matched provider info
4. **Log mapping:** Record infrastructure type and provider UUID

#### **Resource ID Matching**

OCP reports include `resource_id` (e.g., AWS instance ID, Azure VM resource ID):

**OCP Pod Usage:**
```csv
namespace,pod,node,resource_id
koku-app,koku-worker-abc123,ip-10-0-1-100.ec2.internal,i-0abcd1234efgh5678
```

**AWS EC2 Instance:**
```csv
lineItem/ResourceId,lineItem/UsageAccountId,lineItem/UnblendedCost
i-0abcd1234efgh5678,123456789012,1.234
```

**Trino SQL for Matching:**

**Implementation:** See [`reporting_ocpinfrastructure_provider_map.sql`](../../koku/masu/database/trino_sql/) for each cloud provider

**Query Logic:**
- INNER JOIN OCP pod usage with cloud provider line items
- Match on `ocp.resource_id = cloud.resource_identifier`
  - AWS: `lineitem_resourceid` (e.g., `i-0abcd1234efgh5678`)
  - Azure: `instanceid` (Azure VM resource ID)
  - GCP: `resource_name` (GCP instance name)
- Filter by year/month partitions and non-null resource_id
- INSERT DISTINCT mappings into infrastructure provider map table

#### **Cost Allocation to Namespaces**

Once matched, **cloud costs are allocated to OCP namespaces** based on usage:

**Trino SQL for OCP-on-AWS Cost Allocation:**

**Implementation:** See [`reporting_ocpawscostlineitem_project_daily_summary_p.sql`](../../koku/masu/database/trino_sql/aws/openshift/) in `trino_sql/aws/openshift/`

**Query Logic:**
1. **JOIN** OCP daily summary with AWS line items
   - Match on `resource_id` and date
2. **Calculate cost allocation:**
   - Formula: `(pod_effective_usage_cpu_core_hours / node_capacity_cpu_core_hours) * aws_unblended_cost`
   - Proportional share based on CPU usage ratio
3. **Include fields:**
   - OCP: namespace, node, pod metrics, pod_labels
   - AWS: product_code, instance_type, region, tags
4. **Filter:**
   - Both sources must match
   - Only 'Pod' data source
   - Non-null resource_id
5. **INSERT** into `reporting_ocpawscostlineitem_project_daily_summary_p` (partitioned by year/month)

**Result:** Each namespace gets a **proportional share** of the underlying cloud instance cost.

---

## Summary Tables

### **PostgreSQL: `reporting_ocpusagelineitem_daily_summary`**

**Purpose:** Primary table for OCP usage and cost data

**Partitioning:** BY RANGE (`usage_start`) - monthly partitions

**Key Columns:**
- `uuid` (PK), `report_period_id` (FK)
- `cluster_id`, `cluster_alias`
- `data_source` ('Pod' or 'Storage')
- `namespace`, `node`, `resource_id`
- `usage_start`, `usage_end`
- `pod_labels` (JSONB), `volume_labels` (JSONB)
- **Pod metrics:** `pod_usage_cpu_core_hours`, `pod_request_cpu_core_hours`, `pod_effective_usage_cpu_core_hours`, `pod_limit_cpu_core_hours`, `pod_usage_memory_gigabyte_hours`, etc.
- **Node capacity:** `node_capacity_cpu_cores`, `node_capacity_cpu_core_hours`, `node_capacity_memory_gigabytes`, `node_capacity_memory_gigabyte_hours`
- **Cluster capacity:** `cluster_capacity_cpu_core_hours`, `cluster_capacity_memory_gigabyte_hours`
- **Storage metrics:** `persistentvolumeclaim`, `persistentvolume`, `storageclass`, `persistentvolumeclaim_capacity_gigabyte`, `persistentvolumeclaim_usage_gigabyte_months`
- **Cost fields:** `infrastructure_usage_cost` (JSONB), `monthly_cost_type`, `cost_model_cpu_cost`, `cost_model_memory_cost`, `cost_model_volume_cost`
- `cost_category_id` (FK to OpenshiftCostCategory)

**Indexes:**
- `(usage_start)`
- `(namespace)` with varchar_pattern_ops
- `(node)` with varchar_pattern_ops
- `(data_source)`
- `(monthly_cost_type)`
- GIN index on `all_labels`, `pod_labels`, `volume_labels`

**Row Count:** ~1-10M rows/month (depends on cluster size)

### **UI Summary Tables**

For faster API queries, Koku maintains pre-aggregated summary tables:

1. **`reporting_ocp_cost_summary_p`** - Cost aggregated by cluster
2. **`reporting_ocp_cost_summary_by_node_p`** - Cost by node
3. **`reporting_ocp_cost_summary_by_project_p`** - Cost by namespace (project)
4. **`reporting_ocp_pod_summary_p`** - Pod usage aggregated
5. **`reporting_ocp_pod_summary_by_node_p`** - Pod usage by node
6. **`reporting_ocp_pod_summary_by_project_p`** - Pod usage by namespace
7. **`reporting_ocp_volume_summary_p`** - Storage usage aggregated
8. **`reporting_ocp_volume_summary_by_project_p`** - Storage by namespace
9. **`reporting_ocp_network_summary_p`** - Network usage (for OpenShift SDN)
10. **`reporting_ocp_vm_summary_p`** - VM usage (for OpenShift Virtualization)

**Example UI Summary SQL:**

**Implementation:** See [`reporting_ocp_cost_summary_by_project_p.sql`](../../koku/masu/database/trino_sql/openshift/) and related UI summary SQL files

**Query Pattern:**
- **SELECT** aggregated metrics from daily summary table
- **GROUP BY** relevant dimensions (cluster, namespace, date, data_source)
- **SUM** usage metrics (CPU hours, memory hours, storage months)
- **MAX** capacity metrics (node and cluster capacity)
- **SUM** cost metrics (infrastructure costs, cost model costs)
- **INSERT** into pre-aggregated UI summary table (partitioned by year/month)

This pattern is repeated for different aggregation levels:
- By cluster
- By node
- By project (namespace)
- By data source

---

## OCP-Specific Challenges and Solutions

### **1. No Native Cost Data**

**Challenge:** OCP reports only contain usage metrics (CPU seconds, memory bytes), not costs.

**Solution:**
- **Cost Models:** User-defined rates for CPU, memory, storage
- **Tag-Based Pricing:** Assign different rates based on labels (e.g., prod=high, dev=low)
- **Infrastructure Matching:** Match OCP nodes to cloud instances to get actual infrastructure costs
- **Distribution:** Allocate unallocated costs (platform overhead) to workloads

### **2. Multiple Report Types**

**Challenge:** 5 different report types (pod, storage, node labels, namespace labels, VM) must be combined.

**Solution:**
- **Trino SQL:** Complex JOIN across all 5 Parquet tables
- **Label Merging:** `map_concat` to merge pod_labels + node_labels + namespace_labels
- **Data Source Field:** 'Pod' or 'Storage' to distinguish aggregation types

### **3. Effective Usage Calculation**

**Challenge:** Kubernetes charges based on **requests**, not actual usage. But users want to see efficiency.

**Solution:**
- **Effective Usage = min(usage, request)**
- Calculated during CSV preprocessing before Parquet conversion
- Prevents charging for unused requested resources

### **4. Hourly Granularity**

**Challenge:** OCP reports are **hourly** (interval_start to interval_end), cloud reports are **daily**.

**Solution:**
- **Daily Aggregation:** Group by `date(interval_start)` during Trino summarization
- **Unit Conversion:** Seconds → hours for consistency
- **Capacity Tracking:** Track node capacity per day (not per hour)

### **5. Label Filtering**

**Challenge:** OCP clusters can have **thousands** of labels. Querying all labels is slow.

**Solution:**
- **Enabled Tag Keys:** User selects which labels to track via API/UI
- **Trino Filtering:** Use `map_filter` to keep only enabled keys during summarization
  - Convert JSON labels to map
  - Filter keys using `contains(enabled_keys, k)`
  - Only selected labels are stored in summary tables
- **GIN Indexes:** PostgreSQL GIN indexes on JSONB label columns for fast queries

### **6. Operator Version Compatibility**

**Challenge:** Different operator versions send different report formats.

**Solution:**
- **Column Detection:** Dynamically detect columns and add missing ones with appropriate data types
  - Define expected columns with types (e.g., `node`, `csi_driver`, `csi_volume_handle`)
  - Check if columns exist in DataFrame
  - Add missing columns as empty Series with correct dtype
- **Operator Version Tracking:** Store `operator_version` in manifest for compatibility checks
- **Daily Report Detection:** Use `operator_daily_reports` flag to handle old operators that send cumulative reports

**Implementation:** See column detection in [`ocp_report_parquet_processor.py`](../../koku/masu/processor/ocp/ocp_report_parquet_processor.py)

### **7. Infrastructure Matching Race Condition**

**Challenge:** OCP data may arrive **before** cloud provider data for the same day.

**Solution:**
- **Deferred Matching:** If no cloud data found, schedule retry
- **Crossover Processing:** Re-process previous days when new cloud data arrives
- **Infrastructure Flag:** `ocp_on_cloud_updated_datetime` tracks when matching was last attempted

### **8. Multi-Cluster Support**

**Challenge:** One organization may have **multiple OpenShift clusters**.

**Solution:**
- **Cluster ID:** Every record has `cluster_id` (UUID from cluster)
- **Cluster Alias:** User-friendly name for display
- **Cost Category:** Optional grouping of clusters for chargeback

---

## Performance Characteristics

### **Processing Metrics**

| Metric                          | Typical Value   | Notes                                  |
| ------------------------------- | --------------- | -------------------------------------- |
| **Kafka Message Processing**    | 10-30 seconds   | Download + extract + split             |
| **CSV Splitting**               | 5-15 seconds    | Pandas groupby interval_start          |
| **Parquet Conversion**          | 5-20 minutes    | All 5 report types                     |
| **Trino Summarization**         | 3-15 minutes    | Multi-table JOIN + aggregation         |
| **Cost Model Application**      | 2-8 minutes     | PostgreSQL UPDATE with CASE statements |
| **Infrastructure Matching**     | 5-20 minutes    | Trino JOIN between OCP and cloud data  |
| **UI Summary Population**       | 1-5 minutes     | PostgreSQL INSERT from daily summary   |
| **Total Pipeline (daily)**      | 20-60 minutes   | For 1 day of data from 1 cluster       |
| **Initial Ingest (full month)** | 2-8 hours       | For 30 days from 1 cluster             |
| **Line Items per Day**          | 10K - 1M        | Depends on number of pods/PVCs         |
| **Summary Rows per Day**        | 1K - 100K       | ~10-100x reduction                     |
| **Parquet Size**                | 5-50 MB/day     | All 5 report types combined            |
| **PostgreSQL Storage**          | 50-500 MB/month | Daily summary + UI summaries           |

### **Scalability Considerations**

**Cluster Size Impact:**
- **Small cluster (10 nodes, 100 pods):** ~10K rows/day
- **Medium cluster (50 nodes, 500 pods):** ~50K rows/day
- **Large cluster (200 nodes, 2000 pods):** ~200K rows/day
- **Very large cluster (1000 nodes, 10000 pods):** ~1M rows/day

**Multi-Cluster:**
- Processing is **parallel per cluster** (separate Kafka messages)
- PostgreSQL partitions are **shared** (partitioned by date, not cluster)
- Cost model application runs **per cluster**

---

## Data Flow Diagram

```
┌───────────────────────────────────────────────────────────────────────────┐
│  Customer OpenShift Cluster                                               │
│  ┌─────────────────────────────────────────────────────────────────────┐  │
│  │  cost-mgmt-metrics-operator                                         │  │
│  │  - Queries Prometheus (every 6 hours by default, configurable)      │  │
│  │  - Generates 5 CSV reports: pod, storage, node labels, namespace, VM│  │
│  │  - Creates manifest.json                                            │  │
│  │  - Packages as tar.gz                                               │  │
│  └───────────────────────────┬─────────────────────────────────────────┘  │
└──────────────────────────────┼────────────────────────────────────────────┘
                               │ HTTPS POST
                               ▼
┌────────────────────────────────────────────────────────────────────────┐
│  Red Hat Ingress Service                                               │
│  - Stores tar.gz in quarantine S3                                      │
│  - Publishes to Kafka: platform.upload.announce                        │
└───────────────────────────┬────────────────────────────────────────────┘
                            │ Kafka message
                            ▼
┌────────────────────────────────────────────────────────────────────────┐
│  MASU Kafka Consumer (kafka_msg_handler.py)                            │
│  1. Download tar.gz from quarantine S3                                 │
│  2. Extract tar.gz → CSV files + manifest.json                         │
│  3. Parse manifest, create CostUsageReportManifest                     │
│  4. Split CSVs by day (divide_csv_daily)                               │
│  5. Upload daily CSVs to org S3 bucket                                 │
└───────────────────────────┬────────────────────────────────────────────┘
                            │ Trigger processing
                            ▼
┌────────────────────────────────────────────────────────────────────────┐
│  Parquet Processor (ocp_report_parquet_processor.py)                   │
│  For each report type (pod, storage, node_labels, namespace, vm):      │
│  - Read CSV from S3                                                    │
│  - Calculate effective usage (min of usage and request)                │
│  - Add missing columns for new operator versions                       │
│  - Convert to Parquet with proper types                                │
│  - Upload to Trino S3 (partitioned by source/year/month/day)           │
│  - Create OCPUsageReportPeriod in PostgreSQL                           │
└───────────────────────────┬────────────────────────────────────────────┘
                            │ All reports converted
                            ▼
┌────────────────────────────────────────────────────────────────────────┐
│  Trino Summary Updater (ocp_report_parquet_summary_updater.py)         │
│  1. Ensure PostgreSQL partitions exist                                 │
│  2. Query all 5 Parquet tables in Trino                                │
│  3. JOIN pod + storage + node_labels + namespace_labels + VM           │
│  4. Merge labels (pod + node + namespace)                              │
│  5. Aggregate by day (interval_start → usage_start)                    │
│  6. Convert units (seconds → hours, bytes → GB)                        │
│  7. Calculate capacity (node and cluster level)                        │
│  8. INSERT into reporting_ocpusagelineitem_daily_summary               │
│  9. Populate UI summary tables                                         │
│  10. Populate label summary tables                                     │
│  11. Check for cloud infrastructure (resource_id matching)             │
└───────────────────────────┬────────────────────────────────────────────┘
                            │ If cost model exists
                            ▼
┌────────────────────────────────────────────────────────────────────────┐
│  Cost Model Updater (ocp_cost_model_cost_updater.py)                   │
│  1. Load cost model (infrastructure + supplementary + tag rates)       │
│  2. Generate dynamic SQL CASE statements for tag-based pricing         │
│  3. UPDATE daily_summary with:                                         │
│     - cost_model_cpu_cost                                              │
│     - cost_model_memory_cost                                           │
│     - cost_model_volume_cost                                           │
│     - monthly_cost_type (Node, Cluster, PVC, Tag, etc.)                │
│  4. Distribute unallocated costs to namespaces                         │
│  5. Update UI summary tables with costs                                │
└───────────────────────────┬────────────────────────────────────────────┘
                            │ If OCP on cloud infrastructure
                            ▼
┌────────────────────────────────────────────────────────────────────────┐
│  OCP-Cloud Summary Updater (ocp_cloud_parquet_summary_updater.py)      │
│  1. Match resource_id to cloud instances (AWS/Azure/GCP)               │
│  2. JOIN OCP daily_summary with cloud line items                       │
│  3. Allocate cloud costs to namespaces (usage ratio)                   │
│  4. INSERT into reporting_ocpawscostlineitem_project_daily_summary_p   │
│     (or Azure/GCP equivalent)                                          │
│  5. Populate OCP-on-cloud UI summary tables                            │
│  6. Tag matching for unified cost allocation                           │
└───────────────────────────┬────────────────────────────────────────────┘
                            │ Processing complete
                            ▼
┌─────────────────────────────────────────────────────────────────────────┐
│  PostgreSQL                                                             │
│  - reporting_ocpusagelineitem_daily_summary (partitioned by month)      │
│  - reporting_ocp_cost_summary_p (by cluster)                            │
│  - reporting_ocp_cost_summary_by_node_p                                 │
│  - reporting_ocp_cost_summary_by_project_p                              │
│  - reporting_ocp_pod_summary_p                                          │
│  - reporting_ocp_volume_summary_p                                       │
│  - reporting_ocpawscostlineitem_project_daily_summary_p (OCP on AWS)    │
│  - reporting_ocpazurecostlineitem_project_daily_summary_p (OCP on Azure)│
│  - reporting_ocpgcpcostlineitem_project_daily_summary_p (OCP on GCP)    │
└─────────────────────────────────────────────────────────────────────────┘
```

---

## Comparison: OCP vs Cloud Providers

| Aspect                   | OCP                                                   | AWS/Azure/GCP                 |
| ------------------------ | ----------------------------------------------------- | ----------------------------- |
| **Data Source**          | Operator on customer cluster                          | Cloud provider billing export |
| **Collection**           | Push (Kafka)                                          | Pull (download)               |
| **Cost Data**            | **Not included** - calculated via cost models         | Included in billing data      |
| **Report Types**         | **5 types:** pod, storage, node labels, namespace, VM | 1 unified billing file        |
| **Granularity**          | Hourly                                                | Hourly/daily                  |
| **Label Sources**        | **3 sources:** pod + node + namespace                 | Single tags/labels field      |
| **Capacity Tracking**    | **Required:** node + cluster capacity                 | Not applicable                |
| **Effective Usage**      | **min(usage, request)**                               | Actual usage only             |
| **Infrastructure Match** | **OCP-on-AWS/Azure/GCP** via resource_id              | N/A                           |
| **Tag-Based Pricing**    | **Yes** - via cost models                             | Cloud provider pricing only   |
| **Cost Distribution**    | **CPU/Memory/PVC** distribution                       | Not applicable                |
| **Operator Versioning**  | **Complex:** different formats per version            | Cloud provider manages format |
| **Processing Steps**     | 6 phases                                              | 5 phases                      |

---

## Testing OCP Processing

### **Test Data Generation**

For testing OCP processing, generate synthetic CSV reports with realistic data:

**Structure:**
- Create hourly intervals for specified date range
- Generate multiple pods across namespaces and nodes
- Include CPU/memory usage, requests, limits
- Add node capacity metrics
- Include JSON labels for testing label processing
- Set node roles and resource IDs

**Output:** CSV file with proper OCP report structure for testing pipeline

### **Key Test Scenarios**

1. **Multi-report type processing:** Ensure all 5 report types are handled
2. **Daily splitting:** Verify CSVs are split correctly by interval_start
3. **Label merging:** Test pod + node + namespace label combination
4. **Effective usage calculation:** Verify min(usage, request)
5. **Capacity aggregation:** Node and cluster capacity correctness
6. **Cost model application:** Tag-based pricing calculation
7. **Infrastructure matching:** OCP-on-AWS/Azure/GCP cost allocation
8. **Operator version compatibility:** Old vs new column formats
9. **Empty file handling:** Skip empty reports gracefully
10. **Crossover processing:** Handle delayed cloud data

---

## Monitoring and Troubleshooting

### **Key Metrics to Monitor**

- **Kafka lag:** `hccm-group` consumer lag on `platform.upload.announce`
- **Processing time:** Time from Kafka message to summarization complete
- **Parquet file sizes:** Unusually large files may indicate data issues
- **Row counts:** Compare CSV rows → Parquet rows → summary rows
- **Cost model errors:** Tag-based pricing failures
- **Infrastructure matching rate:** % of OCP nodes matched to cloud instances
- **Empty reports:** Frequency of empty CSV files
- **Operator version distribution:** Track which operator versions are active

### **Common Issues**

**Issue:** Missing cost data for OCP cluster
**Cause:** No cost model defined for provider
**Fix:** Create cost model via API or UI

**Issue:** OCP-on-AWS summary tables are empty
**Cause:** resource_id not matching between OCP and AWS data
**Fix:** Verify node resource_id format matches AWS instance ID

**Issue:** Label queries are slow
**Cause:** Too many enabled tag keys
**Fix:** Disable unused tag keys, ensure GIN indexes exist

**Issue:** Processing stuck at "Waiting for all files"
**Cause:** Operator sending daily reports but not all files received yet
**Fix:** Wait for all daily files (manifest.num_total_files)

---

## Code Locations

### **Primary Files**

- **Kafka Message Handler:** `koku/masu/external/kafka_msg_handler.py`
- **Parquet Processor:** `koku/masu/processor/ocp/ocp_report_parquet_processor.py`
- **Summary Updater:** `koku/masu/processor/ocp/ocp_report_parquet_summary_updater.py`
- **Cost Model Updater:** `koku/masu/processor/ocp/ocp_cost_model_cost_updater.py`
- **OCP-Cloud Updater:** `koku/masu/processor/ocp/ocp_cloud_parquet_summary_updater.py`
- **OCP Utilities:** `koku/masu/util/ocp/common.py`

### **SQL Files**

- **Daily Summary (Trino):** `koku/masu/database/trino_sql/openshift/reporting_ocpusagelineitem_daily_summary.sql`
- **Cost Model (PostgreSQL):** `koku/masu/database/sql/openshift/cost_model/*.sql`
- **OCP-AWS Matching (Trino):** `koku/masu/database/trino_sql/aws/openshift/populate_daily_summary/*.sql`
- **OCP-Azure Matching (Trino):** `koku/masu/database/trino_sql/azure/openshift/populate_daily_summary/*.sql`
- **OCP-GCP Matching (Trino):** `koku/masu/database/trino_sql/gcp/openshift/populate_daily_summary/*.sql`

### **Models**

- **OCP Models:** `koku/reporting/provider/ocp/models.py`
- **OCP-AWS Models:** `koku/reporting/provider/aws/openshift/models.py`
- **OCP-Azure Models:** `koku/reporting/provider/azure/openshift/models.py`
- **OCP-GCP Models:** `koku/reporting/provider/gcp/openshift/models.py`

---

## Document Metadata

- **Last Updated:** 2025-01-21
- **Koku Version:** Current
- **Author:** Architecture Documentation Team
