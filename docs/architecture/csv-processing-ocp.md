# OpenShift (OCP) Usage Data Processing Deep Dive

## Overview

OpenShift Cost Management (OCP) data processing is **fundamentally different** from AWS, Azure, and GCP because it uses a **push model** rather than a pull model. Data is generated by the **cost-mgmt-metrics-operator** running on customer OpenShift clusters and pushed to Red Hat's ingress service via Kafka, rather than being downloaded from cloud storage.

This architecture document provides a comprehensive deep-dive into how Koku processes OpenShift usage data, including:

- How the operator collects metrics from clusters
- Kafka-based data ingestion
- Multiple report type handling (pods, storage, node labels, namespace labels, VMs)
- Cost model application and tag-based cost allocation
- Infrastructure matching (OCP on AWS/Azure/GCP)
- Data summarization and reporting

---

## Key Architectural Differences

### **OCP vs Cloud Provider Data Sources**

| Aspect                   | AWS/Azure/GCP          | OpenShift (OCP)                                       |
| ------------------------ | ---------------------- | ----------------------------------------------------- |
| **Data Source**          | Cloud provider storage | **Customer cluster (Prometheus metrics)**             |
| **Collection Method**    | Billing export         | **cost-mgmt-metrics-operator**                        |
| **Data Transmission**    | Download (pull)        | **Upload via HTTPS POST to Ingress (push)**           |
| **File Format**          | CSV in cloud storage   | **tar.gz with CSV files uploaded to ingress**         |
| **Manifest**             | JSON in cloud storage  | **JSON embedded in tar.gz**                           |
| **Update Frequency**     | Daily/hourly export    | **Every 6 hours (configurable)**                      |
| **Report Types**         | Single billing file    | **5 types: pod, storage, node labels, namespace, VM** |
| **Cost Information**     | Included in data       | **Applied via cost models**                           |
| **Infrastructure Costs** | Direct billing         | **Matched to cloud provider via resource ID**         |
| **Authentication**       | Cloud credentials      | **Sources integration or basic auth**                 |
| **Processing Trigger**   | Scheduled download     | **Kafka message consumption**                         |

**Key Insight:** OCP data is **usage-based** (CPU seconds, memory bytes) not cost-based. Costs are calculated using **cost models** and **infrastructure matching**.

---

## OpenShift Operator Architecture

```
┌──────────────────────────────────────────────────────────────────┐
│  Customer OpenShift Cluster                                       │
│  ┌─────────────────────────────────────────────────────────────┐ │
│  │  Prometheus                                                  │ │
│  │  - Collects pod CPU/memory usage                            │ │
│  │  - Collects storage (PVC) usage                             │ │
│  │  - Collects node capacity metrics                           │ │
│  │  - Stores metrics with labels                               │ │
│  └────────────────┬────────────────────────────────────────────┘ │
│                   │ PromQL queries                                │
│                   ▼                                                │
│  ┌─────────────────────────────────────────────────────────────┐ │
│  │  cost-mgmt-metrics-operator                                 │ │
│  │  - Queries Prometheus for usage metrics                     │ │
│  │  - Generates CSV reports (every 6 hours by default)         │ │
│  │  - Creates manifest.json with cluster info                  │ │
│  │  - Packages as tar.gz                                       │ │
│  │  - Uploads to Red Hat Ingress Service                       │ │
│  └────────────────┬────────────────────────────────────────────┘ │
└────────────────────┼────────────────────────────────────────────┘
                     │ HTTPS POST (tar.gz upload)
                     ▼
┌──────────────────────────────────────────────────────────────────┐
│  Red Hat Ingress Service (platform.redhat.com)                   │
│  - Receives uploaded tar.gz                                      │
│  - Stores in quarantine bucket                                   │
│  - Publishes message to Kafka (platform.upload.announce topic)   │
└────────────────┬─────────────────────────────────────────────────┘
                 │ Kafka message with download URL
                 ▼
┌──────────────────────────────────────────────────────────────────┐
│  Koku/MASU (Consumer)                                            │
│  kafka_msg_handler.py                                            │
│  - Consumes messages from platform.upload.announce               │
│  - Downloads tar.gz from quarantine bucket                       │
│  - Extracts and processes CSV files                              │
│  - Sends validation confirmation back to Kafka                   │
└──────────────────────────────────────────────────────────────────┘
```

---

## Report Types and Structure

OpenShift generates **5 distinct report types** (not a single unified billing file like cloud providers):

### **1. Pod Usage Report** (`pod_usage`)

**Purpose:** CPU and memory usage for pods

**Columns:**
- `report_period_start`, `report_period_end`
- `interval_start`, `interval_end` (hourly interval)
- `namespace`, `pod`, `node`, `resource_id` (EC2/Azure/GCP instance ID)
- `pod_usage_cpu_core_seconds`, `pod_request_cpu_core_seconds`, `pod_limit_cpu_core_seconds`
- `pod_usage_memory_byte_seconds`, `pod_request_memory_byte_seconds`, `pod_limit_memory_byte_seconds`
- `node_capacity_cpu_cores`, `node_capacity_cpu_core_seconds`
- `node_capacity_memory_bytes`, `node_capacity_memory_byte_seconds`
- `pod_labels` (JSON)
- `node_role` (new in operator 4.x)

**Example Row:**
```csv
report_period_start,report_period_end,interval_start,interval_end,namespace,pod,node,resource_id,pod_usage_cpu_core_seconds,pod_request_cpu_core_seconds,pod_limit_cpu_core_seconds,pod_usage_memory_byte_seconds,pod_request_memory_byte_seconds,pod_limit_memory_byte_seconds,node_capacity_cpu_cores,node_capacity_cpu_core_seconds,node_capacity_memory_bytes,node_capacity_memory_byte_seconds,pod_labels,node_role
2025-01-01,2025-02-01,2025-01-15T00:00:00Z,2025-01-15T01:00:00Z,koku-app,koku-worker-7d5f8b9c6d-abc123,ip-10-0-1-100.ec2.internal,i-0abcd1234efgh5678,1234.56,3600.00,7200.00,12345678900,26843545600,53687091200,8,28800,32212254720,115964116992000,"{""app"":""koku"",""env"":""prod""}",worker
```

### **2. Storage Usage Report** (`storage_usage`)

**Purpose:** Persistent volume claim (PVC) usage

**Columns:**
- `report_period_start`, `report_period_end`
- `interval_start`, `interval_end`
- `namespace`, `pod`
- `persistentvolumeclaim`, `persistentvolume`, `storageclass`
- `persistentvolumeclaim_capacity_bytes`, `persistentvolumeclaim_capacity_byte_seconds`
- `volume_request_storage_byte_seconds`, `persistentvolumeclaim_usage_byte_seconds`
- `persistentvolume_labels` (JSON), `persistentvolumeclaim_labels` (JSON)
- `csi_driver`, `csi_volume_handle` (new in operator 4.x)
- `node` (new in operator 4.x)

**Example Row:**
```csv
report_period_start,report_period_end,interval_start,interval_end,namespace,pod,persistentvolumeclaim,persistentvolume,storageclass,persistentvolumeclaim_capacity_bytes,persistentvolumeclaim_capacity_byte_seconds,volume_request_storage_byte_seconds,persistentvolumeclaim_usage_byte_seconds,persistentvolume_labels,persistentvolumeclaim_labels,node,csi_driver,csi_volume_handle
2025-01-01,2025-02-01,2025-01-15T00:00:00Z,2025-01-15T01:00:00Z,koku-app,koku-db-0,postgres-data,pvc-abc123,gp3,107374182400,386547056640000,386547056640000,53687091200000,"{""storage.k8s.io/name"":""gp3""}","{""app"":""postgres""}",ip-10-0-1-100.ec2.internal,ebs.csi.aws.com,vol-0123456789abcdef
```

### **3. Node Labels Report** (`node_labels`)

**Purpose:** Labels attached to nodes

**Columns:**
- `report_period_start`, `report_period_end`
- `interval_start`, `interval_end`
- `node`
- `node_labels` (JSON)

### **4. Namespace Labels Report** (`namespace_labels`)

**Purpose:** Labels attached to namespaces

**Columns:**
- `report_period_start`, `report_period_end`
- `interval_start`, `interval_end`
- `namespace`
- `namespace_labels` (JSON)

### **5. VM Usage Report** (`vm_usage`) [OpenShift Virtualization]

**Purpose:** Virtual machine CPU, memory, disk usage (for kubevirt VMs)

**Columns:**
- `report_period_start`, `report_period_end`
- `interval_start`, `interval_end`
- `namespace`, `node`, `resource_id`
- `vm_uptime_total_seconds`
- `vm_cpu_limit_cores`, `vm_cpu_limit_core_seconds`
- `vm_cpu_request_cores`, `vm_cpu_request_core_seconds`
- `vm_cpu_usage_total_seconds`
- `vm_memory_limit_bytes`, `vm_memory_limit_byte_seconds`
- `vm_memory_request_bytes`, `vm_memory_request_byte_seconds`
- `vm_memory_usage_byte_seconds`
- `vm_disk_allocated_size_byte_seconds`

---

## Data Flow Pipeline

### **Phase 1: Kafka Message Reception and Download**

**File:** `koku/masu/external/kafka_msg_handler.py`

#### **Step 1: Consume Kafka Message**

```python
def listen_for_messages_loop():
    """Consume messages from platform.upload.announce topic."""
    kafka_conf = {
        "group.id": "hccm-group",
        "queued.max.messages.kbytes": 1024,
        "enable.auto.commit": False,
        "max.poll.interval.ms": 1080000,  # 18 minutes
    }
    consumer = get_consumer(kafka_conf)
    consumer.subscribe([UPLOAD_TOPIC])  # "platform.upload.announce"

    for _ in itertools.count():
        msg = consumer.poll(timeout=1.0)
        if msg and not msg.error():
            listen_for_messages(msg, consumer)
```

**Kafka Message Format:**
```json
{
  "request_id": "abc123-def456-ghi789",
  "account": "1234567",
  "org_id": "7654321",
  "category": "hccm",
  "url": "https://insights-quarantine.s3.amazonaws.com/...",
  "b64_identity": "eyJpZGVudGl0eSI6IHsuLi59fQ==",
  "timestamp": "2025-01-15T12:34:56.789Z",
  "size": 1234567
}
```

#### **Step 2: Download Tar.gz from Quarantine Bucket**

```python
def download_payload(request_id, url, context):
    """Download the payload from ingress to temporary location."""
    temp_dir = tempfile.mkdtemp(dir=Config.DATA_DIR)

    try:
        download_response = requests.get(url)
        download_response.raise_for_status()
    except requests.exceptions.HTTPError as err:
        shutil.rmtree(temp_dir)
        raise KafkaMsgHandlerError(f"Unable to download file. Error: {str(err)}")

    sanitized_request_id = re.sub("[^A-Za-z0-9]+", "", request_id)
    temp_file = Path(temp_dir, sanitized_request_id).with_suffix(".tar.gz")
    temp_file.write_bytes(download_response.content)

    return temp_file
```

#### **Step 3: Extract Tar.gz and Parse Manifest**

```python
def extract_payload_contents(request_id, tarball_path, context):
    """Extract the payload contents into a temporary location."""
    try:
        mytar = TarFile.open(tarball_path, mode="r:gz")
        mytar.extractall(path=tarball_path.parent)
        files = mytar.getnames()
        manifest_path = [manifest for manifest in files if "manifest.json" in manifest]
    except (ReadError, EOFError, OSError) as error:
        shutil.rmtree(tarball_path.parent)
        raise KafkaMsgHandlerError("Extraction failure.")

    if not manifest_path:
        raise KafkaMsgHandlerError("No manifest found in payload.")

    return manifest_path[0]
```

**Manifest Structure:**
```json
{
  "uuid": "abc123-def456-ghi789",
  "cluster_id": "12345678-90ab-cdef-1234-567890abcdef",
  "date": "2025-01-15T12:00:00.000Z",
  "start": "2025-01-15T12:00:00.000Z",
  "files": [
    "pod_usage_2025-01-15.csv",
    "storage_usage_2025-01-15.csv",
    "node_labels_2025-01-15.csv",
    "namespace_labels_2025-01-15.csv"
  ],
  "certified": true,
  "operator_version": "4.1.0",
  "daily_reports": true,
  "cr_status": {
    "clusterVersion": "4.15",
    "upload": {"upload": true},
    "authentication": {"type": "token"}
  }
}
```

---

### **Phase 2: CSV Splitting and Daily Archives**

**File:** `koku/masu/external/kafka_msg_handler.py`

OpenShift reports can contain **multiple days** of data. Koku splits them into **daily CSV files** for consistency with cloud provider processing.

#### **Step 1: Detect Report Type**

```python
def detect_type(file_path):
    """Detect OCP report type from CSV columns."""
    df = pd.read_csv(file_path, nrows=1)
    columns = set(df.columns)

    if columns.intersection(CPU_MEM_USAGE_COLUMNS) == CPU_MEM_USAGE_COLUMNS:
        return OCPReportTypes.CPU_MEM_USAGE, "pod_usage"
    elif columns.intersection(STORAGE_COLUMNS) == STORAGE_COLUMNS:
        return OCPReportTypes.STORAGE, "storage_usage"
    elif "node_labels" in columns:
        return OCPReportTypes.NODE_LABELS, "node_labels"
    elif "namespace_labels" in columns:
        return OCPReportTypes.NAMESPACE_LABELS, "namespace_labels"
    elif "vm_uptime_total_seconds" in columns:
        return OCPReportTypes.VM_USAGE, "vm_usage"

    return OCPReportTypes.UNKNOWN, "unknown"
```

#### **Step 2: Split by Daily Interval**

```python
def divide_csv_daily(file_path, manifest_id, hour_dict):
    """Split local file into daily content."""
    data_frame = pd.read_csv(file_path, dtype=pd.StringDtype(storage="pyarrow"))

    report_type, _ = utils.detect_type(file_path)

    # Group by day from interval_start column
    unique_times = data_frame.interval_start.unique()
    days = list({cur_dt[:10] for cur_dt in unique_times})  # Extract YYYY-MM-DD

    daily_data_frames = [
        {"data_frame": data_frame[data_frame.interval_start.str.contains(cur_day)], "date": cur_day}
        for cur_day in days
    ]

    daily_files = []
    for daily_data in daily_data_frames:
        day = daily_data["date"]
        df = daily_data["data_frame"]

        # Create unique filename with counter to avoid collisions
        file_prefix = f"{report_type}.{day}.{manifest_id}"
        with transaction.atomic():
            manifest = CostUsageReportManifest.objects.select_for_update().get(id=manifest_id)
            if not manifest.report_tracker.get(file_prefix):
                manifest.report_tracker[file_prefix] = 0
            counter = manifest.report_tracker[file_prefix]
            manifest.report_tracker[file_prefix] = counter + 1
            manifest.save(update_fields=["report_tracker"])

        day_file = f"{file_prefix}.{counter}.csv"
        day_filepath = file_path.parent.joinpath(day_file)
        df.to_csv(day_filepath, index=False, header=True)

        daily_files.append({
            "filepath": day_filepath,
            "date": datetime.strptime(day, "%Y-%m-%d"),
            "num_hours": len(df.interval_start.unique())
        })

    return daily_files
```

#### **Step 3: Upload Daily CSVs to S3/MinIO**

```python
def create_daily_archives(payload_info, filepath, context):
    """Create daily CSVs from incoming report and archive to S3."""
    manifest = payload_info.manifest
    cur_manifest = CostUsageReportManifest.objects.get(id=manifest.manifest_id)

    # Check if operator sends daily reports or if we need to split
    if cur_manifest.operator_version and not cur_manifest.operator_daily_reports:
        # Old operators: additive reports, cannot be split
        daily_files = [{"filepath": filepath, "date": manifest.date, "num_hours": 0}]
    else:
        # New operators or metering: split by day
        daily_files = divide_csv_daily(filepath, manifest.manifest_id, manifest.hours_per_day)

    daily_file_names = {}
    for daily_file in daily_files:
        # Push to S3/MinIO
        s3_csv_path = get_path_prefix(
            payload_info.trino_schema,
            payload_info.provider_type,
            payload_info.provider_uuid,
            daily_file.get("date"),
            Config.CSV_DATA_TYPE,
        )
        filepath = daily_file.get("filepath")
        copy_local_report_file_to_s3_bucket(
            payload_info.request_id,
            s3_csv_path,
            filepath,
            filepath.name,
            manifest.manifest_id,
            context,
        )
        daily_file_names[filepath] = {
            "meta_reportdatestart": str(daily_file["date"].date()),
            "meta_reportnumhours": str(daily_file["num_hours"])
        }

    return daily_file_names
```

**S3/MinIO Path Structure:**
```
org1234567/openshift/csv/
├── pod_usage.2025-01-15.12345.0.csv
├── pod_usage.2025-01-15.12345.1.csv
├── storage_usage.2025-01-15.12345.0.csv
├── node_labels.2025-01-15.12345.0.csv
├── namespace_labels.2025-01-15.12345.0.csv
└── vm_usage.2025-01-15.12345.0.csv
```

---

### **Phase 3: Parquet Conversion**

**File:** `koku/masu/processor/ocp/ocp_report_parquet_processor.py`

#### **Data Type Definitions**

```python
class OCPReportParquetProcessor(ReportParquetProcessorBase):
    def __init__(self, manifest_id, account, s3_path, provider_uuid, parquet_local_path, report_type):
        # Select appropriate Trino table based on report type
        if "daily" in s3_path:
            ocp_table_name = TRINO_LINE_ITEM_TABLE_DAILY_MAP[report_type]
            # e.g., "openshift_pod_usage_line_items_daily"
        else:
            ocp_table_name = TRINO_LINE_ITEM_TABLE_MAP[report_type]
            # e.g., "openshift_pod_usage_line_items"

        # Define numeric columns for proper type conversion
        numeric_columns = [
            "pod_usage_cpu_core_seconds",
            "pod_request_cpu_core_seconds",
            "pod_effective_usage_cpu_core_seconds",
            "pod_limit_cpu_core_seconds",
            "pod_usage_memory_byte_seconds",
            "pod_request_memory_byte_seconds",
            "pod_effective_usage_memory_byte_seconds",
            "pod_limit_memory_byte_seconds",
            "node_capacity_cpu_cores",
            "node_capacity_cpu_core_seconds",
            "node_capacity_memory_bytes",
            "node_capacity_memory_byte_seconds",
            "persistentvolumeclaim_usage_byte_seconds",
            "volume_request_storage_byte_seconds",
            "persistentvolumeclaim_capacity_byte_seconds",
            "persistentvolumeclaim_capacity_bytes",
            # VM metrics
            "vm_uptime_total_seconds",
            "vm_cpu_limit_cores",
            "vm_cpu_limit_core_seconds",
            "vm_cpu_usage_total_seconds",
            "vm_memory_limit_bytes",
            "vm_memory_usage_byte_seconds",
            "vm_disk_allocated_size_byte_seconds",
        ]

        date_columns = ["report_period_start", "report_period_end", "interval_start", "interval_end"]

        column_types = {
            "numeric_columns": numeric_columns,
            "date_columns": date_columns,
            "boolean_columns": []
        }

        super().__init__(
            manifest_id=manifest_id,
            account=account,
            s3_path=s3_path,
            provider_uuid=provider_uuid,
            parquet_local_path=parquet_local_path,
            column_types=column_types,
            table_name=ocp_table_name,
        )
```

#### **Calculate Effective Usage**

Unlike cloud providers, OCP must calculate **effective usage** (min of usage and request):

```python
# In CSV preprocessing (utils.py)
def calculate_effective_usage(df):
    """Calculate effective usage as min(usage, request)."""
    if "pod_usage_cpu_core_seconds" in df.columns and "pod_request_cpu_core_seconds" in df.columns:
        df["pod_effective_usage_cpu_core_seconds"] = df[
            ["pod_usage_cpu_core_seconds", "pod_request_cpu_core_seconds"]
        ].min(axis=1, numeric_only=True)

    if "pod_usage_memory_byte_seconds" in df.columns and "pod_request_memory_byte_seconds" in df.columns:
        df["pod_effective_usage_memory_byte_seconds"] = df[
            ["pod_usage_memory_byte_seconds", "pod_request_memory_byte_seconds"]
        ].min(axis=1, numeric_only=True)

    return df
```

#### **Create Bill Record**

```python
def create_bill(self, bill_date):
    """Create bill postgres entry (OCPUsageReportPeriod)."""
    if isinstance(bill_date, str):
        bill_date = ciso8601.parse_datetime(bill_date)

    report_date_range = month_date_range(bill_date)
    start_date, end_date = report_date_range.split("-")

    report_period_start = ciso8601.parse_datetime(start_date).replace(hour=0, minute=0, tzinfo=settings.UTC)
    report_period_end = ciso8601.parse_datetime(end_date).replace(hour=0, minute=0, tzinfo=settings.UTC)
    # Make end date first of next month
    report_period_end = report_period_end + datetime.timedelta(days=1)

    provider = self._get_provider()
    cluster_id = utils.get_cluster_id_from_provider(provider.uuid)
    cluster_alias = utils.get_cluster_alias_from_cluster_id(cluster_id)

    with schema_context(self._schema_name):
        bill, _ = OCPUsageReportPeriod.objects.get_or_create(
            cluster_id=cluster_id,
            report_period_start=report_period_start,
            report_period_end=report_period_end,
            provider_id=provider.uuid,
        )
        if bill.cluster_alias != cluster_alias:
            bill.cluster_alias = cluster_alias
            bill.save(update_fields=["cluster_alias"])

    return bill
```

**Parquet Output Structure:**
```
org1234567/openshift/parquet/
├── openshift_pod_usage_line_items_daily/
│   ├── source=ocp-provider-uuid/
│   │   ├── year=2025/
│   │   │   ├── month=01/
│   │   │   │   ├── day=15/
│   │   │   │   │   ├── part-00000.parquet
│   │   │   │   │   └── part-00001.parquet
├── openshift_storage_usage_line_items_daily/
│   └── source=ocp-provider-uuid/...
├── openshift_node_labels_line_items_daily/...
├── openshift_namespace_labels_line_items_daily/...
└── openshift_vm_usage_line_items_daily/...
```

---

### **Phase 4: Trino Summarization**

**File:** `koku/masu/processor/ocp/ocp_report_parquet_summary_updater.py`
**SQL:** `koku/masu/database/trino_sql/openshift/reporting_ocpusagelineitem_daily_summary.sql`

#### **Orchestration**

```python
def update_summary_tables(self, start_date, end_date, **kwargs):
    """Populate the summary tables for reporting."""
    start_date, end_date = self._get_sql_inputs(start_date, end_date)
    start_date, end_date = self._check_parquet_date_range(start_date, end_date)

    with schema_context(self._schema):
        self._handle_partitions(self._schema, UI_SUMMARY_TABLES, start_date, end_date)

    with OCPReportDBAccessor(self._schema) as accessor:
        with schema_context(self._schema):
            report_period = accessor.report_periods_for_provider_uuid(self._provider.uuid, start_date)
            if not report_period:
                LOG.warning("No report period found")
                return start_date, end_date
            report_period_id = report_period.id

        # Populate cluster information tables (for cluster-level metadata)
        accessor.populate_openshift_cluster_information_tables(
            self._provider, self._cluster_id, self._cluster_alias, start_date, end_date
        )

        # Process in date ranges
        for start, end in date_range_pair(start_date, end_date, step=settings.TRINO_DATE_STEP):
            # Delete existing summary data (excluding infrastructure costs from OCP-on-cloud matching)
            accessor.delete_all_except_infrastructure_raw_cost_from_daily_summary(
                self._provider.uuid, report_period_id, start, end
            )

            # Populate daily summary table via Trino
            accessor.populate_line_item_daily_summary_table_trino(
                start, end, report_period_id, self._cluster_id, self._cluster_alias, self._provider.uuid
            )

            # Populate UI summary tables
            accessor.populate_ui_summary_tables(start, end, self._provider.uuid)

        # Populate label summary tables
        accessor.populate_pod_label_summary_table([report_period_id], start_date, end_date)
        accessor.populate_volume_label_summary_table([report_period_id], start_date, end_date)

        # Apply tag mappings
        accessor.update_line_item_daily_summary_with_tag_mapping(start_date, end_date, [report_period_id])

        # Update report period timestamps
        if report_period.summary_data_creation_datetime is None:
            report_period.summary_data_creation_datetime = timezone.now()
        report_period.summary_data_updated_datetime = timezone.now()
        report_period.save()

        # Check for cloud infrastructure (triggers OCP-on-AWS/Azure/GCP processing)
        self.check_cluster_infrastructure(start_date, end_date)

    return start_date, end_date
```

#### **Trino SQL: Multi-Report Aggregation**

The Trino SQL joins **all 5 report types** to create a unified daily summary:

```sql
-- reporting_ocpusagelineitem_daily_summary.sql
WITH cte_pg_enabled_keys AS (
    -- Get enabled tag keys from PostgreSQL
    SELECT array['vm_kubevirt_io_name'] || array_agg(key ORDER BY key) AS keys
    FROM postgres.{{schema | sqlsafe}}.reporting_enabledtagkeys
    WHERE enabled = TRUE AND provider_type = 'OCP'
),

cte_ocp_node_label_line_item_daily AS (
    -- Get node labels (filtered by enabled keys)
    SELECT date(nli.interval_start) AS usage_start,
        nli.node,
        CAST(
            map_filter(
                CAST(json_parse(nli.node_labels) AS map(varchar, varchar)),
                (k, v) -> contains(pek.keys, k)
            ) AS json
        ) AS node_labels
    FROM hive.{{schema | sqlsafe}}.openshift_node_labels_line_items_daily AS nli
    CROSS JOIN cte_pg_enabled_keys AS pek
    WHERE nli.source = {{source}}
      AND nli.year = {{year}}
      AND nli.month = {{month}}
      AND nli.interval_start >= {{start_date}}
      AND nli.interval_start < date_add('day', 1, {{end_date}})
    GROUP BY date(nli.interval_start), nli.node, 3
),

cte_ocp_namespace_label_line_item_daily AS (
    -- Get namespace labels (filtered by enabled keys)
    SELECT date(nli.interval_start) AS usage_start,
        nli.namespace,
        CAST(
            map_filter(
                CAST(json_parse(nli.namespace_labels) AS map(varchar, varchar)),
                (k, v) -> contains(pek.keys, k)
            ) AS json
        ) AS namespace_labels
    FROM hive.{{schema | sqlsafe}}.openshift_namespace_labels_line_items_daily AS nli
    CROSS JOIN cte_pg_enabled_keys AS pek
    WHERE nli.source = {{source}}
      AND nli.year = {{year}}
      AND nli.month = {{month}}
      AND nli.interval_start >= {{start_date}}
      AND nli.interval_start < date_add('day', 1, {{end_date}})
    GROUP BY date(nli.interval_start), nli.namespace, 3
),

cte_ocp_node_capacity AS (
    -- Daily sum of node CPU and memory capacity
    SELECT date(nc.interval_start) AS usage_start,
        nc.node,
        SUM(nc.node_capacity_cpu_core_seconds) AS node_capacity_cpu_core_seconds,
        SUM(nc.node_capacity_memory_byte_seconds) AS node_capacity_memory_byte_seconds
    FROM (
        -- Get capacity from pod usage reports
        SELECT li.interval_start, li.node,
            li.node_capacity_cpu_core_seconds,
            li.node_capacity_memory_byte_seconds
        FROM hive.{{schema | sqlsafe}}.openshift_pod_usage_line_items_daily AS li
        WHERE li.source = {{source}}
          AND li.year = {{year}}
          AND li.month = {{month}}
          AND li.interval_start >= {{start_date}}
          AND li.interval_start < date_add('day', 1, {{end_date}})
        GROUP BY li.interval_start, li.node,
            li.node_capacity_cpu_core_seconds,
            li.node_capacity_memory_byte_seconds
    ) AS nc
    GROUP BY date(nc.interval_start), nc.node
),

cte_ocp_cluster_capacity AS (
    -- Daily sum of cluster-wide capacity
    SELECT date(nc.interval_start) AS usage_start,
        SUM(nc.node_capacity_cpu_core_seconds) AS cluster_capacity_cpu_core_seconds,
        SUM(nc.node_capacity_memory_byte_seconds) AS cluster_capacity_memory_byte_seconds
    FROM (
        SELECT li.interval_start, li.node,
            li.node_capacity_cpu_core_seconds,
            li.node_capacity_memory_byte_seconds
        FROM hive.{{schema | sqlsafe}}.openshift_pod_usage_line_items_daily AS li
        WHERE li.source = {{source}}
          AND li.year = {{year}}
          AND li.month = {{month}}
          AND li.interval_start >= {{start_date}}
          AND li.interval_start < date_add('day', 1, {{end_date}})
        GROUP BY li.interval_start, li.node,
            li.node_capacity_cpu_core_seconds,
            li.node_capacity_memory_byte_seconds
    ) AS nc
    GROUP BY date(nc.interval_start)
),

cte_ocp_pod_usage AS (
    -- Aggregate pod CPU and memory usage by day
    SELECT date(li.interval_start) AS usage_start,
        li.namespace,
        li.node,
        li.pod,
        li.resource_id,
        CAST(json_parse(li.pod_labels) AS json) AS pod_labels,
        SUM(li.pod_usage_cpu_core_seconds) / 3600.0 AS pod_usage_cpu_core_hours,
        SUM(li.pod_request_cpu_core_seconds) / 3600.0 AS pod_request_cpu_core_hours,
        SUM(li.pod_effective_usage_cpu_core_seconds) / 3600.0 AS pod_effective_usage_cpu_core_hours,
        SUM(li.pod_limit_cpu_core_seconds) / 3600.0 AS pod_limit_cpu_core_hours,
        SUM(li.pod_usage_memory_byte_seconds) / 3600.0 / 1073741824.0 AS pod_usage_memory_gigabyte_hours,
        SUM(li.pod_request_memory_byte_seconds) / 3600.0 / 1073741824.0 AS pod_request_memory_gigabyte_hours,
        SUM(li.pod_effective_usage_memory_byte_seconds) / 3600.0 / 1073741824.0 AS pod_effective_usage_memory_gigabyte_hours,
        SUM(li.pod_limit_memory_byte_seconds) / 3600.0 / 1073741824.0 AS pod_limit_memory_gigabyte_hours
    FROM hive.{{schema | sqlsafe}}.openshift_pod_usage_line_items_daily AS li
    WHERE li.source = {{source}}
      AND li.year = {{year}}
      AND li.month = {{month}}
      AND li.interval_start >= {{start_date}}
      AND li.interval_start < date_add('day', 1, {{end_date}})
    GROUP BY date(li.interval_start), li.namespace, li.node, li.pod, li.resource_id, 6
),

cte_ocp_storage_usage AS (
    -- Aggregate storage usage by day
    SELECT date(li.interval_start) AS usage_start,
        li.namespace,
        li.node,
        li.persistentvolumeclaim,
        li.persistentvolume,
        li.storageclass,
        li.csi_volume_handle,
        CAST(json_parse(li.persistentvolume_labels) AS json) AS persistentvolume_labels,
        CAST(json_parse(li.persistentvolumeclaim_labels) AS json) AS persistentvolumeclaim_labels,
        MAX(li.persistentvolumeclaim_capacity_bytes) / 1073741824.0 AS persistentvolumeclaim_capacity_gigabyte,
        SUM(li.persistentvolumeclaim_capacity_byte_seconds) / 3600.0 / 1073741824.0 / 730.0 AS persistentvolumeclaim_capacity_gigabyte_months,
        SUM(li.volume_request_storage_byte_seconds) / 3600.0 / 1073741824.0 / 730.0 AS volume_request_storage_gigabyte_months,
        SUM(li.persistentvolumeclaim_usage_byte_seconds) / 3600.0 / 1073741824.0 / 730.0 AS persistentvolumeclaim_usage_gigabyte_months
    FROM hive.{{schema | sqlsafe}}.openshift_storage_usage_line_items_daily AS li
    WHERE li.source = {{source}}
      AND li.year = {{year}}
      AND li.month = {{month}}
      AND li.interval_start >= {{start_date}}
      AND li.interval_start < date_add('day', 1, {{end_date}})
    GROUP BY date(li.interval_start), li.namespace, li.node,
        li.persistentvolumeclaim, li.persistentvolume, li.storageclass,
        li.csi_volume_handle, 8, 9
)

-- Main INSERT: Join all CTEs to create unified daily summary
INSERT INTO hive.{{schema | sqlsafe}}.reporting_ocpusagelineitem_daily_summary (
    uuid, report_period_id, cluster_id, cluster_alias,
    data_source, usage_start, usage_end,
    namespace, node, resource_id,
    pod_labels,
    pod_usage_cpu_core_hours, pod_request_cpu_core_hours,
    pod_effective_usage_cpu_core_hours, pod_limit_cpu_core_hours,
    pod_usage_memory_gigabyte_hours, pod_request_memory_gigabyte_hours,
    pod_effective_usage_memory_gigabyte_hours, pod_limit_memory_gigabyte_hours,
    node_capacity_cpu_cores, node_capacity_cpu_core_hours,
    node_capacity_memory_gigabytes, node_capacity_memory_gigabyte_hours,
    cluster_capacity_cpu_core_hours, cluster_capacity_memory_gigabyte_hours,
    persistentvolumeclaim, persistentvolume, storageclass,
    volume_labels,
    persistentvolumeclaim_capacity_gigabyte,
    persistentvolumeclaim_capacity_gigabyte_months,
    volume_request_storage_gigabyte_months,
    persistentvolumeclaim_usage_gigabyte_months,
    source_uuid, csi_volume_handle,
    source, year, month, day
)
SELECT CAST(uuid() AS varchar) AS uuid,
    {{report_period_id}} AS report_period_id,
    {{cluster_id}} AS cluster_id,
    {{cluster_alias}} AS cluster_alias,

    -- Determine data_source (Pod or Storage)
    CASE
        WHEN pu.pod IS NOT NULL THEN 'Pod'
        WHEN su.persistentvolumeclaim IS NOT NULL THEN 'Storage'
    END AS data_source,

    COALESCE(pu.usage_start, su.usage_start) AS usage_start,
    COALESCE(pu.usage_start, su.usage_start) AS usage_end,

    COALESCE(pu.namespace, su.namespace) AS namespace,
    COALESCE(pu.node, su.node) AS node,
    pu.resource_id,

    -- Merge pod labels with node labels and namespace labels
    CAST(
        map_concat(
            map_concat(
                COALESCE(CAST(pu.pod_labels AS map(varchar, varchar)), map()),
                COALESCE(CAST(nl.node_labels AS map(varchar, varchar)), map())
            ),
            COALESCE(CAST(nsl.namespace_labels AS map(varchar, varchar)), map())
        ) AS json
    ) AS pod_labels,

    -- Pod metrics
    pu.pod_usage_cpu_core_hours,
    pu.pod_request_cpu_core_hours,
    pu.pod_effective_usage_cpu_core_hours,
    pu.pod_limit_cpu_core_hours,
    pu.pod_usage_memory_gigabyte_hours,
    pu.pod_request_memory_gigabyte_hours,
    pu.pod_effective_usage_memory_gigabyte_hours,
    pu.pod_limit_memory_gigabyte_hours,

    -- Node capacity
    nc.node_capacity_cpu_cores / 24.0 AS node_capacity_cpu_cores,
    nc.node_capacity_cpu_core_seconds / 3600.0 AS node_capacity_cpu_core_hours,
    nc.node_capacity_memory_bytes / 1073741824.0 AS node_capacity_memory_gigabytes,
    nc.node_capacity_memory_byte_seconds / 3600.0 / 1073741824.0 AS node_capacity_memory_gigabyte_hours,

    -- Cluster capacity
    cc.cluster_capacity_cpu_core_seconds / 3600.0 AS cluster_capacity_cpu_core_hours,
    cc.cluster_capacity_memory_byte_seconds / 3600.0 / 1073741824.0 AS cluster_capacity_memory_gigabyte_hours,

    -- Storage metrics
    su.persistentvolumeclaim,
    su.persistentvolume,
    su.storageclass,

    -- Merge volume labels
    CAST(
        map_concat(
            COALESCE(CAST(su.persistentvolume_labels AS map(varchar, varchar)), map()),
            COALESCE(CAST(su.persistentvolumeclaim_labels AS map(varchar, varchar)), map())
        ) AS json
    ) AS volume_labels,

    su.persistentvolumeclaim_capacity_gigabyte,
    su.persistentvolumeclaim_capacity_gigabyte_months,
    su.volume_request_storage_gigabyte_months,
    su.persistentvolumeclaim_usage_gigabyte_months,

    {{source}} AS source_uuid,
    su.csi_volume_handle,

    -- Partitioning columns
    {{source}} AS source,
    {{year}} AS year,
    {{month}} AS month,
    CAST(substr(CAST(COALESCE(pu.usage_start, su.usage_start) AS varchar), 9, 2) AS varchar) AS day

FROM cte_ocp_pod_usage AS pu
FULL OUTER JOIN cte_ocp_storage_usage AS su
    ON pu.usage_start = su.usage_start
    AND pu.namespace = su.namespace
    AND pu.node = su.node
LEFT JOIN cte_ocp_node_label_line_item_daily AS nl
    ON COALESCE(pu.usage_start, su.usage_start) = nl.usage_start
    AND COALESCE(pu.node, su.node) = nl.node
LEFT JOIN cte_ocp_namespace_label_line_item_daily AS nsl
    ON COALESCE(pu.usage_start, su.usage_start) = nsl.usage_start
    AND COALESCE(pu.namespace, su.namespace) = nsl.namespace
LEFT JOIN cte_ocp_node_capacity AS nc
    ON COALESCE(pu.usage_start, su.usage_start) = nc.usage_start
    AND COALESCE(pu.node, su.node) = nc.node
LEFT JOIN cte_ocp_cluster_capacity AS cc
    ON COALESCE(pu.usage_start, su.usage_start) = cc.usage_start
;
```

**Key Features:**
1. **Multi-source Join:** Combines pod, storage, node labels, namespace labels
2. **Label Merging:** Merges pod_labels + node_labels + namespace_labels using `map_concat`
3. **Unit Conversion:** Seconds → hours, bytes → gigabytes
4. **Effective Usage:** min(usage, request)
5. **Capacity Aggregation:** Node-level and cluster-level
6. **Data Source Detection:** 'Pod' or 'Storage' based on which data is present

---

### **Phase 5: Cost Model Application**

**File:** `koku/masu/processor/ocp/ocp_cost_model_cost_updater.py`

Unlike cloud providers, **OCP has no cost data in the raw reports**. Costs are calculated using **cost models** defined by users.

#### **Cost Model Structure**

Cost models consist of **infrastructure rates** and **supplementary rates**:

**Infrastructure Rates** (mapped to cloud provider costs):
```json
{
  "cpu_core_usage_per_hour": 0.05,
  "memory_gb_usage_per_hour": 0.01,
  "storage_gb_usage_per_month": 0.10
}
```

**Supplementary Rates** (additional cluster costs):
```json
{
  "node_cost_per_month": 100.0,
  "cluster_cost_per_month": 10000.0,
  "pvc_cost_per_month": 50.0
}
```

**Tag-Based Rates** (assign costs based on labels):
```json
{
  "env": {
    "prod": {"rate": 0.10},
    "dev": {"rate": 0.05},
    "default_value": 0.03
  },
  "team": {
    "platform": {"rate": 0.08},
    "app": {"rate": 0.06}
  }
}
```

#### **Cost Calculation SQL Generation**

The cost updater generates **dynamic SQL CASE statements** for tag-based pricing:

```python
def _build_node_tag_cost_case_statements(self, rate_dict, start_date, default_rate_dict={}, unallocated=False):
    """Generate CASE SQL for tag-based monthly costs."""
    cpu_distribution_term = """
        sum(pod_effective_usage_cpu_core_hours) / max(node_capacity_cpu_core_hours)
    """
    memory_distribution_term = """
        sum(pod_effective_usage_memory_gigabyte_hours) / max(node_capacity_memory_gigabyte_hours)
    """

    case_dict = {}
    for tag_key, tag_value_rates in rate_dict.items():
        cpu_statement_list = ["CASE"]
        memory_statement_list = ["CASE"]

        for tag_value, rate_value in tag_value_rates.items():
            label_condition = f"pod_labels->>'{tag_key}'='{tag_value}'"
            rate = get_amortized_monthly_cost_model_rate(rate_value, start_date)

            cpu_statement_list.append(f"""
                WHEN {label_condition}
                    THEN {cpu_distribution_term} * {rate}::decimal
            """)
            memory_statement_list.append(f"""
                WHEN {label_condition}
                    THEN {memory_distribution_term} * {rate}::decimal
            """)

        # Default rate for tags with the key but no matching value
        if default_rate_dict:
            rate_value = default_rate_dict.get(tag_key, {}).get("default_value", 0)
            rate = get_amortized_monthly_cost_model_rate(rate_value, start_date)
            cpu_statement_list.append(f"""
                ELSE {cpu_distribution_term} * {rate}::decimal
            """)
            memory_statement_list.append(f"""
                ELSE {memory_distribution_term} * {rate}::decimal
            """)

        cpu_statement_list.append("END as cost_model_cpu_cost")
        memory_statement_list.append("END as cost_model_memory_cost")

        case_dict[tag_key] = (
            "\n".join(cpu_statement_list),
            "\n".join(memory_statement_list),
            "0::decimal as cost_model_volume_cost"
        )

    return case_dict
```

**Generated SQL Example:**
```sql
-- Populate monthly cost for tag: env=prod/dev
UPDATE reporting_ocpusagelineitem_daily_summary AS lids
SET
    monthly_cost_type = 'Tag',
    cost_model_cpu_cost = CASE
        WHEN pod_labels->>'env' = 'prod' THEN
            sum(pod_effective_usage_cpu_core_hours) / max(node_capacity_cpu_core_hours) * 0.10::decimal
        WHEN pod_labels->>'env' = 'dev' THEN
            sum(pod_effective_usage_cpu_core_hours) / max(node_capacity_cpu_core_hours) * 0.05::decimal
        ELSE
            sum(pod_effective_usage_cpu_core_hours) / max(node_capacity_cpu_core_hours) * 0.03::decimal
    END,
    cost_model_memory_cost = CASE
        WHEN pod_labels->>'env' = 'prod' THEN
            sum(pod_effective_usage_memory_gigabyte_hours) / max(node_capacity_memory_gigabyte_hours) * 0.10::decimal
        WHEN pod_labels->>'env' = 'dev' THEN
            sum(pod_effective_usage_memory_gigabyte_hours) / max(node_capacity_memory_gigabyte_hours) * 0.05::decimal
        ELSE
            sum(pod_effective_usage_memory_gigabyte_hours) / max(node_capacity_memory_gigabyte_hours) * 0.03::decimal
    END
WHERE usage_start >= '2025-01-01'
  AND usage_start < '2025-02-01'
  AND report_period_id = 12345
  AND pod_labels ? 'env'
;
```

#### **Cost Distribution**

Cost models support **3 distribution types**:

1. **CPU Distribution** (default): Allocate costs based on CPU usage ratio
2. **Memory Distribution**: Allocate costs based on memory usage ratio
3. **PVC Distribution**: Allocate storage costs

**SQL for Cost Distribution:**
```sql
-- Distribute unallocated CPU costs to namespaces
UPDATE reporting_ocpusagelineitem_daily_summary
SET infrastructure_project_cpu_core_hours = (
    pod_effective_usage_cpu_core_hours / node_capacity_cpu_core_hours
) * (
    SELECT sum(infrastructure_usage_cost::numeric->>'cpu')
    FROM reporting_ocpusagelineitem_daily_summary
    WHERE usage_start = '2025-01-15'
      AND monthly_cost_type IS NULL  -- Unallocated
)
WHERE usage_start = '2025-01-15'
  AND monthly_cost_type = 'Node';
```

---

### **Phase 6: Infrastructure Matching (OCP on AWS/Azure/GCP)**

**File:** `koku/masu/processor/ocp/ocp_cloud_parquet_summary_updater.py`

When an OpenShift cluster runs on cloud infrastructure, Koku **matches OCP nodes to cloud instances** to allocate infrastructure costs.

#### **Infrastructure Mapping Discovery**

```python
def check_cluster_infrastructure(self, start_date, end_date):
    """Check if OCP cluster is running on cloud infrastructure."""
    updater_base = OCPCloudUpdaterBase(self._schema, self._provider, self._manifest)

    # Try to get infrastructure map from provider relationships
    if infra_map := updater_base.get_infra_map_from_providers():
        for ocp_source, infra_tuple in infra_map.items():
            LOG.info(
                f"OCP cluster {ocp_source} is running on cloud infrastructure: "
                f"provider_uuid={infra_tuple[0]}, provider_type={infra_tuple[1]}"
            )
            # Trigger OCP-on-Cloud processing
            trigger_ocp_cloud_summary(ocp_source, infra_tuple[0], infra_tuple[1], start_date, end_date)

    # If no relationship, try to infer from resource_id matching
    elif infra_map := updater_base._generate_ocp_infra_map_from_sql_trino(start_date, end_date):
        for ocp_source, infra_tuple in infra_map.items():
            LOG.info(f"Inferred OCP-on-Cloud from resource_id matching")
            trigger_ocp_cloud_summary(ocp_source, infra_tuple[0], infra_tuple[1], start_date, end_date)
```

#### **Resource ID Matching**

OCP reports include `resource_id` (e.g., AWS instance ID, Azure VM resource ID):

**OCP Pod Usage:**
```csv
namespace,pod,node,resource_id
koku-app,koku-worker-abc123,ip-10-0-1-100.ec2.internal,i-0abcd1234efgh5678
```

**AWS EC2 Instance:**
```csv
lineItem/ResourceId,lineItem/UsageAccountId,lineItem/UnblendedCost
i-0abcd1234efgh5678,123456789012,1.234
```

**Trino SQL for Matching:**
```sql
-- reporting_ocpinfrastructure_provider_map.sql
INSERT INTO hive.{{schema | sqlsafe}}.reporting_ocpinfrastructure_provider_map
SELECT DISTINCT
    ocp.source_uuid AS ocp_uuid,
    aws.source_uuid AS infra_uuid,
    'AWS' AS infra_type
FROM hive.{{schema | sqlsafe}}.openshift_pod_usage_line_items_daily AS ocp
INNER JOIN hive.{{schema | sqlsafe}}.aws_line_items AS aws
    ON ocp.resource_id = aws.lineitem_resourceid
WHERE ocp.year = {{year}}
  AND ocp.month = {{month}}
  AND ocp.resource_id IS NOT NULL
  AND aws.year = {{year}}
  AND aws.month = {{month}}
;
```

#### **Cost Allocation to Namespaces**

Once matched, **cloud costs are allocated to OCP namespaces** based on usage:

**Trino SQL for OCP-on-AWS Cost Allocation:**
```sql
-- reporting_ocpawscostlineitem_project_daily_summary_p.sql
INSERT INTO postgres.{{schema | sqlsafe}}.reporting_ocpawscostlineitem_project_daily_summary_p
SELECT
    uuid() AS uuid,
    ocp.report_period_id,
    aws.cost_entry_bill_id,
    ocp.cluster_id,
    ocp.cluster_alias,
    ocp.namespace AS namespace,
    ocp.node,
    ocp.resource_id,
    ocp.usage_start,
    ocp.usage_end,

    -- AWS cost fields
    aws.product_code,
    aws.product_family,
    aws.instance_type,
    aws.region,
    aws.availability_zone,

    -- Allocate AWS cost to namespace based on CPU usage ratio
    (ocp.pod_effective_usage_cpu_core_hours / ocp.node_capacity_cpu_core_hours)
        * aws.unblended_cost AS pod_cost,

    -- Usage fields
    ocp.pod_usage_cpu_core_hours,
    ocp.pod_request_cpu_core_hours,
    ocp.pod_effective_usage_cpu_core_hours,
    ocp.pod_usage_memory_gigabyte_hours,
    ocp.pod_request_memory_gigabyte_hours,
    ocp.pod_effective_usage_memory_gigabyte_hours,

    -- Labels
    ocp.pod_labels,
    aws.tags,

    -- Partitioning
    cast(year(ocp.usage_start) AS varchar) AS year,
    cast(month(ocp.usage_start) AS varchar) AS month

FROM hive.{{schema | sqlsafe}}.reporting_ocpusagelineitem_daily_summary AS ocp
INNER JOIN hive.{{schema | sqlsafe}}.aws_line_items AS aws
    ON ocp.resource_id = aws.lineitem_resourceid
    AND date(ocp.usage_start) = date(aws.lineitem_usagestartdate)
WHERE ocp.source = {{ocp_source_uuid}}
  AND aws.source = {{aws_source_uuid}}
  AND ocp.year = {{year}}
  AND ocp.month = {{month}}
  AND ocp.usage_start >= {{start_date}}
  AND ocp.usage_start < date_add('day', 1, {{end_date}})
  AND ocp.data_source = 'Pod'
  AND ocp.resource_id IS NOT NULL
;
```

**Result:** Each namespace gets a **proportional share** of the underlying cloud instance cost.

---

## Summary Tables

### **PostgreSQL: `reporting_ocpusagelineitem_daily_summary`**

**Purpose:** Primary table for OCP usage and cost data

**Partitioning:** BY RANGE (`usage_start`) - monthly partitions

**Key Columns:**
- `uuid` (PK), `report_period_id` (FK)
- `cluster_id`, `cluster_alias`
- `data_source` ('Pod' or 'Storage')
- `namespace`, `node`, `resource_id`
- `usage_start`, `usage_end`
- `pod_labels` (JSONB), `volume_labels` (JSONB)
- **Pod metrics:** `pod_usage_cpu_core_hours`, `pod_request_cpu_core_hours`, `pod_effective_usage_cpu_core_hours`, `pod_limit_cpu_core_hours`, `pod_usage_memory_gigabyte_hours`, etc.
- **Node capacity:** `node_capacity_cpu_cores`, `node_capacity_cpu_core_hours`, `node_capacity_memory_gigabytes`, `node_capacity_memory_gigabyte_hours`
- **Cluster capacity:** `cluster_capacity_cpu_core_hours`, `cluster_capacity_memory_gigabyte_hours`
- **Storage metrics:** `persistentvolumeclaim`, `persistentvolume`, `storageclass`, `persistentvolumeclaim_capacity_gigabyte`, `persistentvolumeclaim_usage_gigabyte_months`
- **Cost fields:** `infrastructure_usage_cost` (JSONB), `monthly_cost_type`, `cost_model_cpu_cost`, `cost_model_memory_cost`, `cost_model_volume_cost`
- `cost_category_id` (FK to OpenshiftCostCategory)

**Indexes:**
- `(usage_start)`
- `(namespace)` with varchar_pattern_ops
- `(node)` with varchar_pattern_ops
- `(data_source)`
- `(monthly_cost_type)`
- GIN index on `all_labels`, `pod_labels`, `volume_labels`

**Row Count:** ~1-10M rows/month (depends on cluster size)

### **UI Summary Tables**

For faster API queries, Koku maintains pre-aggregated summary tables:

1. **`reporting_ocp_cost_summary_p`** - Cost aggregated by cluster
2. **`reporting_ocp_cost_summary_by_node_p`** - Cost by node
3. **`reporting_ocp_cost_summary_by_project_p`** - Cost by namespace (project)
4. **`reporting_ocp_pod_summary_p`** - Pod usage aggregated
5. **`reporting_ocp_pod_summary_by_node_p`** - Pod usage by node
6. **`reporting_ocp_pod_summary_by_project_p`** - Pod usage by namespace
7. **`reporting_ocp_volume_summary_p`** - Storage usage aggregated
8. **`reporting_ocp_volume_summary_by_project_p`** - Storage by namespace
9. **`reporting_ocp_network_summary_p`** - Network usage (for OpenShift SDN)
10. **`reporting_ocp_vm_summary_p`** - VM usage (for OpenShift Virtualization)

**Example UI Summary SQL:**
```sql
-- reporting_ocp_cost_summary_by_project_p.sql
INSERT INTO postgres.{{schema | sqlsafe}}.reporting_ocp_cost_summary_by_project_p
SELECT
    cast(uuid() AS varchar) AS uuid,
    usage_start,
    usage_start AS usage_end,
    cluster_id,
    cluster_alias,
    namespace,
    data_source,

    -- Aggregated usage
    sum(pod_usage_cpu_core_hours) AS pod_usage_cpu_core_hours,
    sum(pod_request_cpu_core_hours) AS pod_request_cpu_core_hours,
    sum(pod_effective_usage_cpu_core_hours) AS pod_effective_usage_cpu_core_hours,
    sum(pod_limit_cpu_core_hours) AS pod_limit_cpu_core_hours,

    sum(pod_usage_memory_gigabyte_hours) AS pod_usage_memory_gigabyte_hours,
    sum(pod_request_memory_gigabyte_hours) AS pod_request_memory_gigabyte_hours,
    sum(pod_effective_usage_memory_gigabyte_hours) AS pod_effective_usage_memory_gigabyte_hours,
    sum(pod_limit_memory_gigabyte_hours) AS pod_limit_memory_gigabyte_hours,

    max(node_capacity_cpu_core_hours) AS node_capacity_cpu_core_hours,
    max(node_capacity_memory_gigabyte_hours) AS node_capacity_memory_gigabyte_hours,
    max(cluster_capacity_cpu_core_hours) AS cluster_capacity_cpu_core_hours,
    max(cluster_capacity_memory_gigabyte_hours) AS cluster_capacity_memory_gigabyte_hours,

    sum(persistentvolumeclaim_capacity_gigabyte_months) AS persistentvolumeclaim_capacity_gigabyte_months,
    sum(persistentvolumeclaim_usage_gigabyte_months) AS persistentvolumeclaim_usage_gigabyte_months,

    -- Aggregated costs
    sum(coalesce(infrastructure_raw_cost, 0)) AS infrastructure_raw_cost,
    sum(coalesce(infrastructure_project_raw_cost, 0)) AS infrastructure_project_raw_cost,
    sum(coalesce(cost_model_cpu_cost, 0)) AS cost_model_cpu_cost,
    sum(coalesce(cost_model_memory_cost, 0)) AS cost_model_memory_cost,
    sum(coalesce(cost_model_volume_cost, 0)) AS cost_model_volume_cost,

    source_uuid,
    cast(year(usage_start) AS varchar) AS year,
    lpad(cast(month(usage_start) AS varchar), 2, '0') AS month

FROM postgres.{{schema | sqlsafe}}.reporting_ocpusagelineitem_daily_summary
WHERE usage_start >= {{start_date}}
  AND usage_start < date_add('day', 1, {{end_date}})
  AND source_uuid = {{source_uuid}}
GROUP BY
    usage_start, cluster_id, cluster_alias, namespace, data_source, source_uuid, year, month
;
```

---

## OCP-Specific Challenges and Solutions

### **1. No Native Cost Data**

**Challenge:** OCP reports only contain usage metrics (CPU seconds, memory bytes), not costs.

**Solution:**
- **Cost Models:** User-defined rates for CPU, memory, storage
- **Tag-Based Pricing:** Assign different rates based on labels (e.g., prod=high, dev=low)
- **Infrastructure Matching:** Match OCP nodes to cloud instances to get actual infrastructure costs
- **Distribution:** Allocate unallocated costs (platform overhead) to workloads

### **2. Multiple Report Types**

**Challenge:** 5 different report types (pod, storage, node labels, namespace labels, VM) must be combined.

**Solution:**
- **Trino SQL:** Complex JOIN across all 5 Parquet tables
- **Label Merging:** `map_concat` to merge pod_labels + node_labels + namespace_labels
- **Data Source Field:** 'Pod' or 'Storage' to distinguish aggregation types

### **3. Effective Usage Calculation**

**Challenge:** Kubernetes charges based on **requests**, not actual usage. But users want to see efficiency.

**Solution:**
- **Effective Usage = min(usage, request)**
- Calculated during CSV preprocessing before Parquet conversion
- Prevents charging for unused requested resources

### **4. Hourly Granularity**

**Challenge:** OCP reports are **hourly** (interval_start to interval_end), cloud reports are **daily**.

**Solution:**
- **Daily Aggregation:** Group by `date(interval_start)` during Trino summarization
- **Unit Conversion:** Seconds → hours for consistency
- **Capacity Tracking:** Track node capacity per day (not per hour)

### **5. Label Filtering**

**Challenge:** OCP clusters can have **thousands** of labels. Querying all labels is slow.

**Solution:**
- **Enabled Tag Keys:** User selects which labels to track
- **Trino Filtering:** `map_filter` to keep only enabled keys
  ```sql
  map_filter(
      cast(json_parse(pod_labels) as map(varchar, varchar)),
      (k, v) -> contains(enabled_keys, k)
  )
  ```
- **GIN Indexes:** PostgreSQL GIN indexes on JSONB labels for fast queries

### **6. Operator Version Compatibility**

**Challenge:** Different operator versions send different report formats.

**Solution:**
- **Column Detection:** Dynamically detect columns and add missing ones
  ```python
  STORAGE_NEWV_COLUMNS_AND_TYPES = {
      "node": pd.StringDtype(storage="pyarrow"),
      "csi_driver": pd.StringDtype(storage="pyarrow"),
      "csi_volume_handle": pd.StringDtype(storage="pyarrow"),
  }

  def add_new_columns(df):
      for col, dtype in STORAGE_NEWV_COLUMNS_AND_TYPES.items():
          if col not in df.columns:
              df[col] = pd.Series(dtype=dtype)
      return df
  ```
- **Operator Version Tracking:** Store `operator_version` in manifest
- **Daily Report Detection:** Handle old operators that send cumulative reports

### **7. Infrastructure Matching Race Condition**

**Challenge:** OCP data may arrive **before** cloud provider data for the same day.

**Solution:**
- **Deferred Matching:** If no cloud data found, schedule retry
- **Crossover Processing:** Re-process previous days when new cloud data arrives
- **Infrastructure Flag:** `ocp_on_cloud_updated_datetime` tracks when matching was last attempted

### **8. Multi-Cluster Support**

**Challenge:** One organization may have **multiple OpenShift clusters**.

**Solution:**
- **Cluster ID:** Every record has `cluster_id` (UUID from cluster)
- **Cluster Alias:** User-friendly name for display
- **Cost Category:** Optional grouping of clusters for chargeback

---

## Performance Characteristics

### **Processing Metrics**

| Metric                          | Typical Value   | Notes                                  |
| ------------------------------- | --------------- | -------------------------------------- |
| **Kafka Message Processing**    | 10-30 seconds   | Download + extract + split             |
| **CSV Splitting**               | 5-15 seconds    | Pandas groupby interval_start          |
| **Parquet Conversion**          | 5-20 minutes    | All 5 report types                     |
| **Trino Summarization**         | 3-15 minutes    | Multi-table JOIN + aggregation         |
| **Cost Model Application**      | 2-8 minutes     | PostgreSQL UPDATE with CASE statements |
| **Infrastructure Matching**     | 5-20 minutes    | Trino JOIN between OCP and cloud data  |
| **UI Summary Population**       | 1-5 minutes     | PostgreSQL INSERT from daily summary   |
| **Total Pipeline (daily)**      | 20-60 minutes   | For 1 day of data from 1 cluster       |
| **Initial Ingest (full month)** | 2-8 hours       | For 30 days from 1 cluster             |
| **Line Items per Day**          | 10K - 1M        | Depends on number of pods/PVCs         |
| **Summary Rows per Day**        | 1K - 100K       | ~10-100x reduction                     |
| **Parquet Size**                | 5-50 MB/day     | All 5 report types combined            |
| **PostgreSQL Storage**          | 50-500 MB/month | Daily summary + UI summaries           |

### **Scalability Considerations**

**Cluster Size Impact:**
- **Small cluster (10 nodes, 100 pods):** ~10K rows/day
- **Medium cluster (50 nodes, 500 pods):** ~50K rows/day
- **Large cluster (200 nodes, 2000 pods):** ~200K rows/day
- **Very large cluster (1000 nodes, 10000 pods):** ~1M rows/day

**Multi-Cluster:**
- Processing is **parallel per cluster** (separate Kafka messages)
- PostgreSQL partitions are **shared** (partitioned by date, not cluster)
- Cost model application runs **per cluster**

---

## Data Flow Diagram

```
┌───────────────────────────────────────────────────────────────────────────┐
│  Customer OpenShift Cluster                                               │
│  ┌─────────────────────────────────────────────────────────────────────┐  │
│  │  cost-mgmt-metrics-operator                                         │  │
│  │  - Queries Prometheus (every 6 hours by default, configurable)      │  │
│  │  - Generates 5 CSV reports: pod, storage, node labels, namespace, VM│  │
│  │  - Creates manifest.json                                            │  │
│  │  - Packages as tar.gz                                               │  │
│  └───────────────────────────┬─────────────────────────────────────────┘  │
└──────────────────────────────┼────────────────────────────────────────────┘
                               │ HTTPS POST
                               ▼
┌────────────────────────────────────────────────────────────────────────┐
│  Red Hat Ingress Service                                               │
│  - Stores tar.gz in quarantine S3                                      │
│  - Publishes to Kafka: platform.upload.announce                        │
└───────────────────────────┬────────────────────────────────────────────┘
                            │ Kafka message
                            ▼
┌────────────────────────────────────────────────────────────────────────┐
│  MASU Kafka Consumer (kafka_msg_handler.py)                            │
│  1. Download tar.gz from quarantine S3                                 │
│  2. Extract tar.gz → CSV files + manifest.json                         │
│  3. Parse manifest, create CostUsageReportManifest                     │
│  4. Split CSVs by day (divide_csv_daily)                               │
│  5. Upload daily CSVs to org S3 bucket                                 │
└───────────────────────────┬────────────────────────────────────────────┘
                            │ Trigger processing
                            ▼
┌────────────────────────────────────────────────────────────────────────┐
│  Parquet Processor (ocp_report_parquet_processor.py)                   │
│  For each report type (pod, storage, node_labels, namespace, vm):      │
│  - Read CSV from S3                                                    │
│  - Calculate effective usage (min of usage and request)                │
│  - Add missing columns for new operator versions                       │
│  - Convert to Parquet with proper types                                │
│  - Upload to Trino S3 (partitioned by source/year/month/day)           │
│  - Create OCPUsageReportPeriod in PostgreSQL                           │
└───────────────────────────┬────────────────────────────────────────────┘
                            │ All reports converted
                            ▼
┌────────────────────────────────────────────────────────────────────────┐
│  Trino Summary Updater (ocp_report_parquet_summary_updater.py)         │
│  1. Ensure PostgreSQL partitions exist                                 │
│  2. Query all 5 Parquet tables in Trino                                │
│  3. JOIN pod + storage + node_labels + namespace_labels + VM           │
│  4. Merge labels (pod + node + namespace)                              │
│  5. Aggregate by day (interval_start → usage_start)                    │
│  6. Convert units (seconds → hours, bytes → GB)                        │
│  7. Calculate capacity (node and cluster level)                        │
│  8. INSERT into reporting_ocpusagelineitem_daily_summary               │
│  9. Populate UI summary tables                                         │
│  10. Populate label summary tables                                     │
│  11. Check for cloud infrastructure (resource_id matching)             │
└───────────────────────────┬────────────────────────────────────────────┘
                            │ If cost model exists
                            ▼
┌────────────────────────────────────────────────────────────────────────┐
│  Cost Model Updater (ocp_cost_model_cost_updater.py)                   │
│  1. Load cost model (infrastructure + supplementary + tag rates)       │
│  2. Generate dynamic SQL CASE statements for tag-based pricing         │
│  3. UPDATE daily_summary with:                                         │
│     - cost_model_cpu_cost                                              │
│     - cost_model_memory_cost                                           │
│     - cost_model_volume_cost                                           │
│     - monthly_cost_type (Node, Cluster, PVC, Tag, etc.)                │
│  4. Distribute unallocated costs to namespaces                         │
│  5. Update UI summary tables with costs                                │
└───────────────────────────┬────────────────────────────────────────────┘
                            │ If OCP on cloud infrastructure
                            ▼
┌────────────────────────────────────────────────────────────────────────┐
│  OCP-Cloud Summary Updater (ocp_cloud_parquet_summary_updater.py)      │
│  1. Match resource_id to cloud instances (AWS/Azure/GCP)               │
│  2. JOIN OCP daily_summary with cloud line items                       │
│  3. Allocate cloud costs to namespaces (usage ratio)                   │
│  4. INSERT into reporting_ocpawscostlineitem_project_daily_summary_p   │
│     (or Azure/GCP equivalent)                                          │
│  5. Populate OCP-on-cloud UI summary tables                            │
│  6. Tag matching for unified cost allocation                           │
└───────────────────────────┬────────────────────────────────────────────┘
                            │ Processing complete
                            ▼
┌─────────────────────────────────────────────────────────────────────────┐
│  PostgreSQL                                                             │
│  - reporting_ocpusagelineitem_daily_summary (partitioned by month)      │
│  - reporting_ocp_cost_summary_p (by cluster)                            │
│  - reporting_ocp_cost_summary_by_node_p                                 │
│  - reporting_ocp_cost_summary_by_project_p                              │
│  - reporting_ocp_pod_summary_p                                          │
│  - reporting_ocp_volume_summary_p                                       │
│  - reporting_ocpawscostlineitem_project_daily_summary_p (OCP on AWS)    │
│  - reporting_ocpazurecostlineitem_project_daily_summary_p (OCP on Azure)│
│  - reporting_ocpgcpcostlineitem_project_daily_summary_p (OCP on GCP)    │
└─────────────────────────────────────────────────────────────────────────┘
```

---

## Comparison: OCP vs Cloud Providers

| Aspect                   | OCP                                                   | AWS/Azure/GCP                 |
| ------------------------ | ----------------------------------------------------- | ----------------------------- |
| **Data Source**          | Operator on customer cluster                          | Cloud provider billing export |
| **Collection**           | Push (Kafka)                                          | Pull (download)               |
| **Cost Data**            | **Not included** - calculated via cost models         | Included in billing data      |
| **Report Types**         | **5 types:** pod, storage, node labels, namespace, VM | 1 unified billing file        |
| **Granularity**          | Hourly                                                | Hourly/daily                  |
| **Label Sources**        | **3 sources:** pod + node + namespace                 | Single tags/labels field      |
| **Capacity Tracking**    | **Required:** node + cluster capacity                 | Not applicable                |
| **Effective Usage**      | **min(usage, request)**                               | Actual usage only             |
| **Infrastructure Match** | **OCP-on-AWS/Azure/GCP** via resource_id              | N/A                           |
| **Tag-Based Pricing**    | **Yes** - via cost models                             | Cloud provider pricing only   |
| **Cost Distribution**    | **CPU/Memory/PVC** distribution                       | Not applicable                |
| **Operator Versioning**  | **Complex:** different formats per version            | Cloud provider manages format |
| **Processing Steps**     | 6 phases                                              | 5 phases                      |

---

## Testing OCP Processing

### **Test Data Generation**

```python
def create_test_ocp_report(cluster_id, start_date, num_pods=10, num_hours=24):
    """Generate test OCP pod usage CSV."""
    data = []
    for hour in range(num_hours):
        timestamp = start_date + timedelta(hours=hour)
        for pod_num in range(num_pods):
            data.append({
                "report_period_start": start_date.strftime("%Y-%m-%d"),
                "report_period_end": (start_date + timedelta(days=30)).strftime("%Y-%m-%d"),
                "interval_start": timestamp.isoformat() + "Z",
                "interval_end": (timestamp + timedelta(hours=1)).isoformat() + "Z",
                "namespace": f"namespace-{pod_num % 3}",
                "pod": f"pod-{pod_num}",
                "node": f"node-{pod_num % 5}",
                "resource_id": f"i-{pod_num:012d}",
                "pod_usage_cpu_core_seconds": random.uniform(1000, 3000),
                "pod_request_cpu_core_seconds": 3600,
                "pod_limit_cpu_core_seconds": 7200,
                "pod_usage_memory_byte_seconds": random.uniform(1e10, 3e10),
                "pod_request_memory_byte_seconds": 2.68e10,
                "pod_limit_memory_byte_seconds": 5.37e10,
                "node_capacity_cpu_cores": 8,
                "node_capacity_cpu_core_seconds": 28800,
                "node_capacity_memory_bytes": 32212254720,
                "node_capacity_memory_byte_seconds": 115964116992000,
                "pod_labels": json.dumps({"app": f"app-{pod_num % 2}", "env": "prod"}),
                "node_role": "worker"
            })

    df = pd.DataFrame(data)
    df.to_csv(f"pod_usage_{start_date.strftime('%Y-%m-%d')}.csv", index=False)
    return df
```

### **Key Test Scenarios**

1. **Multi-report type processing:** Ensure all 5 report types are handled
2. **Daily splitting:** Verify CSVs are split correctly by interval_start
3. **Label merging:** Test pod + node + namespace label combination
4. **Effective usage calculation:** Verify min(usage, request)
5. **Capacity aggregation:** Node and cluster capacity correctness
6. **Cost model application:** Tag-based pricing calculation
7. **Infrastructure matching:** OCP-on-AWS/Azure/GCP cost allocation
8. **Operator version compatibility:** Old vs new column formats
9. **Empty file handling:** Skip empty reports gracefully
10. **Crossover processing:** Handle delayed cloud data

---

## Monitoring and Troubleshooting

### **Key Metrics to Monitor**

- **Kafka lag:** `hccm-group` consumer lag on `platform.upload.announce`
- **Processing time:** Time from Kafka message to summarization complete
- **Parquet file sizes:** Unusually large files may indicate data issues
- **Row counts:** Compare CSV rows → Parquet rows → summary rows
- **Cost model errors:** Tag-based pricing failures
- **Infrastructure matching rate:** % of OCP nodes matched to cloud instances
- **Empty reports:** Frequency of empty CSV files
- **Operator version distribution:** Track which operator versions are active

### **Common Issues**

**Issue:** Missing cost data for OCP cluster
**Cause:** No cost model defined for provider
**Fix:** Create cost model via API or UI

**Issue:** OCP-on-AWS summary tables are empty
**Cause:** resource_id not matching between OCP and AWS data
**Fix:** Verify node resource_id format matches AWS instance ID

**Issue:** Label queries are slow
**Cause:** Too many enabled tag keys
**Fix:** Disable unused tag keys, ensure GIN indexes exist

**Issue:** Processing stuck at "Waiting for all files"
**Cause:** Operator sending daily reports but not all files received yet
**Fix:** Wait for all daily files (manifest.num_total_files)

---

## Code Locations

### **Primary Files**

- **Kafka Message Handler:** `koku/masu/external/kafka_msg_handler.py`
- **Parquet Processor:** `koku/masu/processor/ocp/ocp_report_parquet_processor.py`
- **Summary Updater:** `koku/masu/processor/ocp/ocp_report_parquet_summary_updater.py`
- **Cost Model Updater:** `koku/masu/processor/ocp/ocp_cost_model_cost_updater.py`
- **OCP-Cloud Updater:** `koku/masu/processor/ocp/ocp_cloud_parquet_summary_updater.py`
- **OCP Utilities:** `koku/masu/util/ocp/common.py`

### **SQL Files**

- **Daily Summary (Trino):** `koku/masu/database/trino_sql/openshift/reporting_ocpusagelineitem_daily_summary.sql`
- **Cost Model (PostgreSQL):** `koku/masu/database/sql/openshift/cost_model/*.sql`
- **OCP-AWS Matching (Trino):** `koku/masu/database/trino_sql/aws/openshift/populate_daily_summary/*.sql`
- **OCP-Azure Matching (Trino):** `koku/masu/database/trino_sql/azure/openshift/populate_daily_summary/*.sql`
- **OCP-GCP Matching (Trino):** `koku/masu/database/trino_sql/gcp/openshift/populate_daily_summary/*.sql`

### **Models**

- **OCP Models:** `koku/reporting/provider/ocp/models.py`
- **OCP-AWS Models:** `koku/reporting/provider/aws/openshift/models.py`
- **OCP-Azure Models:** `koku/reporting/provider/azure/openshift/models.py`
- **OCP-GCP Models:** `koku/reporting/provider/gcp/openshift/models.py`

---

## Document Metadata

- **Last Updated:** 2025-01-21
- **Koku Version:** Current
- **Author:** Architecture Documentation Team
